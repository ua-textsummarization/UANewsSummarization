{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SqzyV06rYiLm"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>intro</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60 хвилин на реакцію. Як ефективно протидіяти ...</td>\n",
       "      <td>Вищі урядовці мають відреагувати на появу воро...</td>\n",
       "      <td>Одрі Танг любить висловлюватися точно. Під час...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Фільм про нас. Cеріал «Чорнобиль» показує вади...</td>\n",
       "      <td>Серіал має найбільший рейтинг серед усіх інших...</td>\n",
       "      <td>«Влада погано комунікує з суспільством», – чує...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Київські візерунки. Зіграйте у гру й перевірте...</td>\n",
       "      <td>У Києві є близько сорока мікрорайонів. Наскіль...</td>\n",
       "      <td>Що бачить художник, коли дивиться на карту Киє...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Протистояння медіа та соцмереж під час виборів...</td>\n",
       "      <td>Вибори президента України в 2019 році були уні...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Модераторка дискусії Діана Дуцик з Могилянсько...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Філарет проти канонічності. Як українські прав...</td>\n",
       "      <td>Колишній патріарх УПЦ КП Філарет — нерозлучний...</td>\n",
       "      <td>Попри це, патріархові Філарету потрібно віддат...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Міліцейський саботаж і суддівська змова. Як ст...</td>\n",
       "      <td>Позови переважно складені за принципом «килимо...</td>\n",
       "      <td>Автор: Ольга Худецька, журналіст, член Атестац...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Моніторинг тем, які піднімає російська дезінфо...</td>\n",
       "      <td>Обмеження дослідження: теми російської пропага...</td>\n",
       "      <td>Російські дезінформаційні видання з окупованих...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"Автономія\" Донбасу в складі України – це крах...</td>\n",
       "      <td>Обрання нового президента з риторикою «про пош...</td>\n",
       "      <td>Втім, саме по собі надання «окремим районам До...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Щоб зупинити реванш, патріотичні партії мають ...</td>\n",
       "      <td>Існує реальна загроза формування однопартійної...</td>\n",
       "      <td>Видавши сумнівний з точки зору законності Указ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>«Мондеґрін». Культовий письменник із Донецька ...</td>\n",
       "      <td>Події в «Мондеґріні» розгортаються в режимі фа...</td>\n",
       "      <td>Прозова новинка, що побачила світ напередодні ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>В Україні і греко-католики, і православні з за...</td>\n",
       "      <td>Їх так багато, що більшість народу сприймає ці...</td>\n",
       "      <td>Гуцульська архітектурна школа, волинська, беса...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Основна причина паводків — забудова заплав. Ру...</td>\n",
       "      <td>Останнім часом увага суспільства прикута до си...</td>\n",
       "      <td>Звісно, водоохоронну роль лісів важко заперечи...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Сім'я без сексу. Радянський тоталітаризм вбива...</td>\n",
       "      <td>Чоловіки, не маючи здорового контакту зі своїм...</td>\n",
       "      <td>В інтерв’ю Текстам я розповіла про те, як трав...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Реваншизм, популізм і прагнення тотальної влад...</td>\n",
       "      <td>Невизначеність зовнішньополітичного курсу, роз...</td>\n",
       "      <td>Найголовніше, навіть не те, що сказав новий Пр...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Зрадофіли, порохоботи та журнашлюхи. Хроніки р...</td>\n",
       "      <td>Останніми роками наш лексикон поповнився купою...</td>\n",
       "      <td>Тексти не раз критикували українську журналіст...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>“Гра престолів”: справжня причина, чому фани н...</td>\n",
       "      <td>Відома дослідниця впливу технологій на суспіль...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Моніторинг тем, які піднімає російська пропага...</td>\n",
       "      <td>Ми починаємо регулярну публікацію моніторингу ...</td>\n",
       "      <td>Короткий висновок: Відповідно до російської ко...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Перетоки голосів між І-м та ІІ-м туром виборів...</td>\n",
       "      <td>Виборці Гриценка з Галичини та Києва дали найб...</td>\n",
       "      <td>Це друга й остання частина статті про те, наск...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Закон про мову: що він регулює і як це працюва...</td>\n",
       "      <td>Документ докладно визначає, як саме захищаєтьс...</td>\n",
       "      <td>Сьогодні, 15 травня, Петро Порошенко підписав ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Who voted for Zelenskiy after all? Most of Tym...</td>\n",
       "      <td>We have no intention to draw psychological por...</td>\n",
       "      <td>In order to simulate the distribution of the v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Чому в Україні зміннюються пори року, а на екв...</td>\n",
       "      <td>Багато хто вважає, що температура на нашій пла...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Руки незрячих. Ними плетуть сталеві канати і ч...</td>\n",
       "      <td>З-під широких чорних окулярів, скам’янілих на ...</td>\n",
       "      <td>З 1 квітня почали діяти нові державні будівель...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Страсті Партріарха. Владні амбіції Філарета шк...</td>\n",
       "      <td>Під час створення Православної церкви України ...</td>\n",
       "      <td>Православна Церква України створена 15 грудня ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Справжній Голобородько. Тезка «Слуги народу» п...</td>\n",
       "      <td>\"Він ворожий до української культури. Його усп...</td>\n",
       "      <td>У них нема нічого спільного, окрім імені й прі...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Король Інстаграму? Ні, продукт телебачення. Хт...</td>\n",
       "      <td>Більшість зірок Інстаграму – дуже оголені та м...</td>\n",
       "      <td>Про цьогорічну президентську виборчу кампанію ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Психологічні травми, отримані в радянські часи...</td>\n",
       "      <td>Коли на особисту травму нашаровується травма с...</td>\n",
       "      <td>— Сьогодні ми живемо у відносно спокійному сві...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Дев'ять причин, чому програв Порошенко, і що й...</td>\n",
       "      <td>Олігархічні телеканали, неякісні розслідування...</td>\n",
       "      <td>Журналісти плакали на фінальній прес-конференц...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Хто ж (все-таки) голосував за Зеленського? Най...</td>\n",
       "      <td>Ми не будемо малювати психологічні портрети чи...</td>\n",
       "      <td>Щоб змоделювати, хто з виборців першого туру  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Бурштинова республіка потрапила в художню літе...</td>\n",
       "      <td>У романі Василя Тибеля «Бурштин» будні нелегал...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>Гроші на бізнес від держави. В розвинених краї...</td>\n",
       "      <td>Банк Англії надає фінансування тим приватним б...</td>\n",
       "      <td>В Україні близько 2 мільйонів малих підприємст...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>Мовні квоти на ТБ. Чому «ліберали» не праві і ...</td>\n",
       "      <td>Менеджери телеканалів, крім упередженого ставл...</td>\n",
       "      <td>Нещодавно ухвалений парламентом Закон «щодо мо...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>«Відкриті дані» держструктур: лише 8% у машино...</td>\n",
       "      <td>На Єдиному державному порталі відкритих даних ...</td>\n",
       "      <td>Держава  збирає і продукує багато інформації, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>Медицина в Грузії: чого досягла радикальна реф...</td>\n",
       "      <td>Багатоповерхова будівля Республіканської лікар...</td>\n",
       "      <td>— Доброго дня. Я журналіст із України. Хочу на...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>Як покращити якість державних закупівель та мі...</td>\n",
       "      <td>Дослідження ґрунтується на аналізі 35 тисяч «п...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Для порівняння старої та нової систем закупіве...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>Перша офіційна компенсація від МВС за дії \"Бер...</td>\n",
       "      <td>Дружина загиблого майданівця передала половину...</td>\n",
       "      <td>Сергій Дідич - 45-річний депутат Городенківськ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>Дев’ять тисяч доларів на даху. Як живеться вла...</td>\n",
       "      <td>Садибу Сергія Попова легко знайти в заплутаних...</td>\n",
       "      <td>Зараз Сергій має 30 панелей, і, для прикладу, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>Розхитати Донбас. Як депутати демократичних си...</td>\n",
       "      <td>Влада зменшує тиск і критику, а регіонали не с...</td>\n",
       "      <td>Революція гідності і війна з Росією розморозил...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>Боротьба з уявною «ЛГБТ-загрозою» стає реальни...</td>\n",
       "      <td>Не має значення, як Ви ставитись до проблем се...</td>\n",
       "      <td>У листопаді минулого року першим тривожним «дз...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>12 міфів \"про Захід\". Все не так, як здається</td>\n",
       "      <td>Коли комусь з українців не подобається якесь у...</td>\n",
       "      <td>У якомусь українському ресторані туалети зачин...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>Чиновникам наплювати. Уряд провалив власні пла...</td>\n",
       "      <td>У Миколаєві на Львівщині завдяки переведенню к...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Держава здатна значно скоротити використання і...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>1000 кілометрів Змієвих валів під Києвом. Хто ...</td>\n",
       "      <td>На півдні від Києва можна зустріти дивні земля...</td>\n",
       "      <td>Про це читайте, вивчайте Вали на картах та див...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>Український експорт до ЄС: сировина, сировина ...</td>\n",
       "      <td>Коли ціни на метал, руду, зерно та олію ростут...</td>\n",
       "      <td>&lt;a href='#'&gt;&lt;img alt='Експорт українських това...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>Черги за біометричними паспортами. Чому більші...</td>\n",
       "      <td>О 6-й ранку біля скляної будівлі сервісного це...</td>\n",
       "      <td>6 квітня 521 голосом «за» Європейський парламе...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>Найнеймовірніші причини відкласти. Огляд судів...</td>\n",
       "      <td>Судячи з тактики, яку повально демонструє у су...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>\"Тюрми СБУ\", Окуєва-не-чеченка, радикали захоп...</td>\n",
       "      <td>Ми прослідкували, як і ким  поширювалися три н...</td>\n",
       "      <td>Це нормально, бо люди не повинні проводити вес...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>Злочини на ґрунті ненависті. Визнати не можна ...</td>\n",
       "      <td>Наприкінці травня активісти націоналістичних у...</td>\n",
       "      <td>Правозахисники, з якими спілкувались ТЕКСТИ, о...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>П'ять куль у лежачого. Справи Майдану: суди по...</td>\n",
       "      <td>Беркутівці з «чорної роти»; командир роти харк...</td>\n",
       "      <td>Загалом існує близько 50 справ Майдану, по біл...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>Бійці колчаківських фронтів. Хто, крім бійців ...</td>\n",
       "      <td>В Україні сотні ветеранських організацій, які ...</td>\n",
       "      <td>Питання — куди йдуть ветеранські гроші, особли...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>Сила землі. Три гектари забезпечують родині до...</td>\n",
       "      <td>Невелика земельна ділянка поблизу Києва забезп...</td>\n",
       "      <td>— Останніми роками стала частіше до церкви ход...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>Міф про 75% української на телебаченні. Все пр...</td>\n",
       "      <td>Закон справді зміцнює позиції української мови...</td>\n",
       "      <td>Прийнятий Верховною Радою законопроект №5313, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>Українці помирають рано. Чому так відбувається...</td>\n",
       "      <td>Наші громадяни живуть на 11 років менше за євр...</td>\n",
       "      <td>Сталося це після Революції  Гідності. Одна моя...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>Євробачення-2017: погуляли на мільярд. Найбіль...</td>\n",
       "      <td>Двісті тридцять мільйонів гривень Київ вклав в...</td>\n",
       "      <td>2016 року Київ перерахував  Суспільному ТБ (ко...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>5 книжок із Арсеналу: від спогадів підпільниці...</td>\n",
       "      <td>Закінчився щорічний Книжковий Арсенал у Києві,...</td>\n",
       "      <td>1. Володимир Єрмоленко. Ловець океану. — Львів...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>Паркувальний армагеддон. Чому право ставити ма...</td>\n",
       "      <td>Як запобігти появі автомобільних заторів, забр...</td>\n",
       "      <td>За інформацією ТЕКСТІВ, київська мерія схиляєт...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>Мільйон з гектара. Дрібні фермерські господарс...</td>\n",
       "      <td>Практикою багатьох країн доведено, що фермерсь...</td>\n",
       "      <td>Володимир Завадовський керує невеликим аграрни...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>План анексії Білорусі. Як Лукашенко поставив с...</td>\n",
       "      <td>Протягом 2017 року російсько-український кордо...</td>\n",
       "      <td>Контракт між Міноборони РФ та залізницею перед...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>Бути іншим. Пісня переможця Євробачення-17 від...</td>\n",
       "      <td>Чому на Євробаченні  переміг саме португалець ...</td>\n",
       "      <td>На Spotify зазначено, що характеристика «енерг...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>516 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "0    60 хвилин на реакцію. Як ефективно протидіяти ...   \n",
       "1    Фільм про нас. Cеріал «Чорнобиль» показує вади...   \n",
       "2    Київські візерунки. Зіграйте у гру й перевірте...   \n",
       "3    Протистояння медіа та соцмереж під час виборів...   \n",
       "4                                                  NaN   \n",
       "5    Філарет проти канонічності. Як українські прав...   \n",
       "6    Міліцейський саботаж і суддівська змова. Як ст...   \n",
       "7    Моніторинг тем, які піднімає російська дезінфо...   \n",
       "8    \"Автономія\" Донбасу в складі України – це крах...   \n",
       "9    Щоб зупинити реванш, патріотичні партії мають ...   \n",
       "10   «Мондеґрін». Культовий письменник із Донецька ...   \n",
       "11   В Україні і греко-католики, і православні з за...   \n",
       "12   Основна причина паводків — забудова заплав. Ру...   \n",
       "13   Сім'я без сексу. Радянський тоталітаризм вбива...   \n",
       "14   Реваншизм, популізм і прагнення тотальної влад...   \n",
       "15   Зрадофіли, порохоботи та журнашлюхи. Хроніки р...   \n",
       "16   “Гра престолів”: справжня причина, чому фани н...   \n",
       "17   Моніторинг тем, які піднімає російська пропага...   \n",
       "18   Перетоки голосів між І-м та ІІ-м туром виборів...   \n",
       "19   Закон про мову: що він регулює і як це працюва...   \n",
       "20   Who voted for Zelenskiy after all? Most of Tym...   \n",
       "21   Чому в Україні зміннюються пори року, а на екв...   \n",
       "22   Руки незрячих. Ними плетуть сталеві канати і ч...   \n",
       "23   Страсті Партріарха. Владні амбіції Філарета шк...   \n",
       "24   Справжній Голобородько. Тезка «Слуги народу» п...   \n",
       "25   Король Інстаграму? Ні, продукт телебачення. Хт...   \n",
       "26   Психологічні травми, отримані в радянські часи...   \n",
       "27   Дев'ять причин, чому програв Порошенко, і що й...   \n",
       "28   Хто ж (все-таки) голосував за Зеленського? Най...   \n",
       "29   Бурштинова республіка потрапила в художню літе...   \n",
       "..                                                 ...   \n",
       "486  Гроші на бізнес від держави. В розвинених краї...   \n",
       "487  Мовні квоти на ТБ. Чому «ліберали» не праві і ...   \n",
       "488  «Відкриті дані» держструктур: лише 8% у машино...   \n",
       "489  Медицина в Грузії: чого досягла радикальна реф...   \n",
       "490  Як покращити якість державних закупівель та мі...   \n",
       "491                                                NaN   \n",
       "492  Перша офіційна компенсація від МВС за дії \"Бер...   \n",
       "493  Дев’ять тисяч доларів на даху. Як живеться вла...   \n",
       "494  Розхитати Донбас. Як депутати демократичних си...   \n",
       "495  Боротьба з уявною «ЛГБТ-загрозою» стає реальни...   \n",
       "496      12 міфів \"про Захід\". Все не так, як здається   \n",
       "497  Чиновникам наплювати. Уряд провалив власні пла...   \n",
       "498                                                NaN   \n",
       "499  1000 кілометрів Змієвих валів під Києвом. Хто ...   \n",
       "500  Український експорт до ЄС: сировина, сировина ...   \n",
       "501  Черги за біометричними паспортами. Чому більші...   \n",
       "502  Найнеймовірніші причини відкласти. Огляд судів...   \n",
       "503  \"Тюрми СБУ\", Окуєва-не-чеченка, радикали захоп...   \n",
       "504  Злочини на ґрунті ненависті. Визнати не можна ...   \n",
       "505  П'ять куль у лежачого. Справи Майдану: суди по...   \n",
       "506  Бійці колчаківських фронтів. Хто, крім бійців ...   \n",
       "507  Сила землі. Три гектари забезпечують родині до...   \n",
       "508  Міф про 75% української на телебаченні. Все пр...   \n",
       "509  Українці помирають рано. Чому так відбувається...   \n",
       "510  Євробачення-2017: погуляли на мільярд. Найбіль...   \n",
       "511  5 книжок із Арсеналу: від спогадів підпільниці...   \n",
       "512  Паркувальний армагеддон. Чому право ставити ма...   \n",
       "513  Мільйон з гектара. Дрібні фермерські господарс...   \n",
       "514  План анексії Білорусі. Як Лукашенко поставив с...   \n",
       "515  Бути іншим. Пісня переможця Євробачення-17 від...   \n",
       "\n",
       "                                                 intro  \\\n",
       "0    Вищі урядовці мають відреагувати на появу воро...   \n",
       "1    Серіал має найбільший рейтинг серед усіх інших...   \n",
       "2    У Києві є близько сорока мікрорайонів. Наскіль...   \n",
       "3    Вибори президента України в 2019 році були уні...   \n",
       "4    Модераторка дискусії Діана Дуцик з Могилянсько...   \n",
       "5    Колишній патріарх УПЦ КП Філарет — нерозлучний...   \n",
       "6    Позови переважно складені за принципом «килимо...   \n",
       "7    Обмеження дослідження: теми російської пропага...   \n",
       "8    Обрання нового президента з риторикою «про пош...   \n",
       "9    Існує реальна загроза формування однопартійної...   \n",
       "10   Події в «Мондеґріні» розгортаються в режимі фа...   \n",
       "11   Їх так багато, що більшість народу сприймає ці...   \n",
       "12   Останнім часом увага суспільства прикута до си...   \n",
       "13   Чоловіки, не маючи здорового контакту зі своїм...   \n",
       "14   Невизначеність зовнішньополітичного курсу, роз...   \n",
       "15   Останніми роками наш лексикон поповнився купою...   \n",
       "16   Відома дослідниця впливу технологій на суспіль...   \n",
       "17   Ми починаємо регулярну публікацію моніторингу ...   \n",
       "18   Виборці Гриценка з Галичини та Києва дали найб...   \n",
       "19   Документ докладно визначає, як саме захищаєтьс...   \n",
       "20   We have no intention to draw psychological por...   \n",
       "21   Багато хто вважає, що температура на нашій пла...   \n",
       "22   З-під широких чорних окулярів, скам’янілих на ...   \n",
       "23   Під час створення Православної церкви України ...   \n",
       "24   \"Він ворожий до української культури. Його усп...   \n",
       "25   Більшість зірок Інстаграму – дуже оголені та м...   \n",
       "26   Коли на особисту травму нашаровується травма с...   \n",
       "27   Олігархічні телеканали, неякісні розслідування...   \n",
       "28   Ми не будемо малювати психологічні портрети чи...   \n",
       "29   У романі Василя Тибеля «Бурштин» будні нелегал...   \n",
       "..                                                 ...   \n",
       "486  Банк Англії надає фінансування тим приватним б...   \n",
       "487  Менеджери телеканалів, крім упередженого ставл...   \n",
       "488  На Єдиному державному порталі відкритих даних ...   \n",
       "489  Багатоповерхова будівля Республіканської лікар...   \n",
       "490  Дослідження ґрунтується на аналізі 35 тисяч «п...   \n",
       "491  Для порівняння старої та нової систем закупіве...   \n",
       "492  Дружина загиблого майданівця передала половину...   \n",
       "493  Садибу Сергія Попова легко знайти в заплутаних...   \n",
       "494  Влада зменшує тиск і критику, а регіонали не с...   \n",
       "495  Не має значення, як Ви ставитись до проблем се...   \n",
       "496  Коли комусь з українців не подобається якесь у...   \n",
       "497  У Миколаєві на Львівщині завдяки переведенню к...   \n",
       "498  Держава здатна значно скоротити використання і...   \n",
       "499  На півдні від Києва можна зустріти дивні земля...   \n",
       "500  Коли ціни на метал, руду, зерно та олію ростут...   \n",
       "501  О 6-й ранку біля скляної будівлі сервісного це...   \n",
       "502  Судячи з тактики, яку повально демонструє у су...   \n",
       "503  Ми прослідкували, як і ким  поширювалися три н...   \n",
       "504  Наприкінці травня активісти націоналістичних у...   \n",
       "505  Беркутівці з «чорної роти»; командир роти харк...   \n",
       "506  В Україні сотні ветеранських організацій, які ...   \n",
       "507  Невелика земельна ділянка поблизу Києва забезп...   \n",
       "508  Закон справді зміцнює позиції української мови...   \n",
       "509  Наші громадяни живуть на 11 років менше за євр...   \n",
       "510  Двісті тридцять мільйонів гривень Київ вклав в...   \n",
       "511  Закінчився щорічний Книжковий Арсенал у Києві,...   \n",
       "512  Як запобігти появі автомобільних заторів, забр...   \n",
       "513  Практикою багатьох країн доведено, що фермерсь...   \n",
       "514  Протягом 2017 року російсько-український кордо...   \n",
       "515  Чому на Євробаченні  переміг саме португалець ...   \n",
       "\n",
       "                                               article  \n",
       "0    Одрі Танг любить висловлюватися точно. Під час...  \n",
       "1    «Влада погано комунікує з суспільством», – чує...  \n",
       "2    Що бачить художник, коли дивиться на карту Киє...  \n",
       "3                                                  NaN  \n",
       "4                                                  NaN  \n",
       "5    Попри це, патріархові Філарету потрібно віддат...  \n",
       "6    Автор: Ольга Худецька, журналіст, член Атестац...  \n",
       "7    Російські дезінформаційні видання з окупованих...  \n",
       "8    Втім, саме по собі надання «окремим районам До...  \n",
       "9    Видавши сумнівний з точки зору законності Указ...  \n",
       "10   Прозова новинка, що побачила світ напередодні ...  \n",
       "11   Гуцульська архітектурна школа, волинська, беса...  \n",
       "12   Звісно, водоохоронну роль лісів важко заперечи...  \n",
       "13   В інтерв’ю Текстам я розповіла про те, як трав...  \n",
       "14   Найголовніше, навіть не те, що сказав новий Пр...  \n",
       "15   Тексти не раз критикували українську журналіст...  \n",
       "16                                                 NaN  \n",
       "17   Короткий висновок: Відповідно до російської ко...  \n",
       "18   Це друга й остання частина статті про те, наск...  \n",
       "19   Сьогодні, 15 травня, Петро Порошенко підписав ...  \n",
       "20   In order to simulate the distribution of the v...  \n",
       "21                                                 NaN  \n",
       "22   З 1 квітня почали діяти нові державні будівель...  \n",
       "23   Православна Церква України створена 15 грудня ...  \n",
       "24   У них нема нічого спільного, окрім імені й прі...  \n",
       "25   Про цьогорічну президентську виборчу кампанію ...  \n",
       "26   — Сьогодні ми живемо у відносно спокійному сві...  \n",
       "27   Журналісти плакали на фінальній прес-конференц...  \n",
       "28   Щоб змоделювати, хто з виборців першого туру  ...  \n",
       "29                                                 NaN  \n",
       "..                                                 ...  \n",
       "486  В Україні близько 2 мільйонів малих підприємст...  \n",
       "487  Нещодавно ухвалений парламентом Закон «щодо мо...  \n",
       "488  Держава  збирає і продукує багато інформації, ...  \n",
       "489  — Доброго дня. Я журналіст із України. Хочу на...  \n",
       "490                                                NaN  \n",
       "491                                                NaN  \n",
       "492  Сергій Дідич - 45-річний депутат Городенківськ...  \n",
       "493  Зараз Сергій має 30 панелей, і, для прикладу, ...  \n",
       "494  Революція гідності і війна з Росією розморозил...  \n",
       "495  У листопаді минулого року першим тривожним «дз...  \n",
       "496  У якомусь українському ресторані туалети зачин...  \n",
       "497                                                NaN  \n",
       "498                                                NaN  \n",
       "499  Про це читайте, вивчайте Вали на картах та див...  \n",
       "500  <a href='#'><img alt='Експорт українських това...  \n",
       "501  6 квітня 521 голосом «за» Європейський парламе...  \n",
       "502                                                NaN  \n",
       "503  Це нормально, бо люди не повинні проводити вес...  \n",
       "504  Правозахисники, з якими спілкувались ТЕКСТИ, о...  \n",
       "505  Загалом існує близько 50 справ Майдану, по біл...  \n",
       "506  Питання — куди йдуть ветеранські гроші, особли...  \n",
       "507  — Останніми роками стала частіше до церкви ход...  \n",
       "508  Прийнятий Верховною Радою законопроект №5313, ...  \n",
       "509  Сталося це після Революції  Гідності. Одна моя...  \n",
       "510  2016 року Київ перерахував  Суспільному ТБ (ко...  \n",
       "511  1. Володимир Єрмоленко. Ловець океану. — Львів...  \n",
       "512  За інформацією ТЕКСТІВ, київська мерія схиляєт...  \n",
       "513  Володимир Завадовський керує невеликим аграрни...  \n",
       "514  Контракт між Міноборони РФ та залізницею перед...  \n",
       "515  На Spotify зазначено, що характеристика «енерг...  \n",
       "\n",
       "[516 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles = pd.read_csv(\"../data/texty_news.csv\")\n",
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title      12\n",
       "intro       0\n",
       "article    47\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = articles.dropna()\n",
    "articles = articles.drop(['intro'], 1)\n",
    "articles = articles.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60 хвилин на реакцію. Як ефективно протидіяти ...</td>\n",
       "      <td>Одрі Танг любить висловлюватися точно. Під час...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Фільм про нас. Cеріал «Чорнобиль» показує вади...</td>\n",
       "      <td>«Влада погано комунікує з суспільством», – чує...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Київські візерунки. Зіграйте у гру й перевірте...</td>\n",
       "      <td>Що бачить художник, коли дивиться на карту Киє...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Філарет проти канонічності. Як українські прав...</td>\n",
       "      <td>Попри це, патріархові Філарету потрібно віддат...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Міліцейський саботаж і суддівська змова. Як ст...</td>\n",
       "      <td>Автор: Ольга Худецька, журналіст, член Атестац...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  60 хвилин на реакцію. Як ефективно протидіяти ...   \n",
       "1  Фільм про нас. Cеріал «Чорнобиль» показує вади...   \n",
       "2  Київські візерунки. Зіграйте у гру й перевірте...   \n",
       "3  Філарет проти канонічності. Як українські прав...   \n",
       "4  Міліцейський саботаж і суддівська змова. Як ст...   \n",
       "\n",
       "                                             article  \n",
       "0  Одрі Танг любить висловлюватися точно. Під час...  \n",
       "1  «Влада погано комунікує з суспільством», – чує...  \n",
       "2  Що бачить художник, коли дивиться на карту Киє...  \n",
       "3  Попри це, патріархові Філарету потрібно віддат...  \n",
       "4  Автор: Ольга Худецька, журналіст, член Атестац...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Фільм про нас. Cеріал «Чорнобиль» показує вади радянської системи, дотепер вкорінені в Україні\n",
      "Article: «Влада погано комунікує з суспільством», – чуємо ми сьогодні. У цього явища багато причин, і одна з них криється у Чорнобильській трагедії. Аварію, причини й наслідки якої попервах погано розуміли навіть у Кремлі, усіляко приховували, применшували. Власне, для СРСР це було «доброю традицією». ЧАЕС не була першою техногенною катастрофою (варто задати хоча б Киштимську аварію на Уралі 1957-го).  Однак масштаб та інформаційний ефект Чорнобиля – нечуваний. Відтоді серед українців залишилася помітна травма: коли трапляється щось надзвичайне, влада вочевидь брехатиме і викривлятиме інформацію. Тому в нашому соціумі є дуже велике поле для чуток і маніпуляцій.  Вже в добу соцмереж нерідко з’являються фейкові повідомлення в стилі «чоловік моєї подруги працює пожежником у Чорнобильські зоні, і він сказав»… – а далі йдуть різні варіації жахалок. Це «заходить» і через 30 років після вибуху на АЕС, бо надто вже цинічною була брехня тоді.  «Брехатимуть і далі», – впевнені мільйони наших громадян і сьогодні. В незалежній Україні немає тотальної цензури, всемогутнього КГБ, імперських амбіцій. Однак чиновникам під час надзвичайних ситуацій доводиться докладати чимало зусиль, щоб викликати бодай якусь довіру до себе. Порівняно недавня історія: 2007-го року біля села Ожидів на Львівщині перекинулися цистерни з фосфором. Генерал Олександр Кузьмук, який був на той час віце-прем’єром, приїхав на місце подій і, щоб заспокоїти місцевих мешканців, скуштував огірок з городу місцевої мешканки.  На перший погляд, типова «ляшковщина». Але й пережиток Чорнобиля також. В однойменному серіалі є промовиста сцена, коли шахтар кидає на стіл перед московськими чиновниками респіратор зі словами: «Якби вони(респіратори) допомагали, ви б самі носили їх, не знімаючи».  Чорнобильська брехня породила недовіру, коли представники влади змушені переконувати людей власним емпіричним прикладом чи бодай персональною присутністю. Торік було масове отруєння дітей в Черкасах, Володимир Гройсман особисто літав їх провідати. Люди молодшого покоління висміяли це у численних фотожабах. А для покоління, яке пережило Чорнобиль, особиста присутність очільника уряду стала переконливим спростуванням неймовірних чуток про екологічну ситуацію в місті.  В серіалі доволі промовисто показано ієрархію радянського чиновництва: дрібні місцеві функціонери прагнуть обдурити столичних «шишок», уникнути відповідальності, зіграти на некомпетентності високих гостей. Коли прип’ятські можновладці зустрічають Бориса Щербину, то впевнені, що зуміють навіщати локшини на вуха цьому кремлівському партійному бонзі.  Їхньому здивуванню немає меж, коли «товариш із ЦК» демонструє їм знання з атомної енергетики. Так нерідко буває й з українськими чиновниками: візитерам із Києва прагнуть продемонструвати показуху, не пускати «далі фасаду», заколисувати їхню увагу, замість ділової розмови перевести візит у банкет і п’янку. Усі ці радянські прихвати можна помітити ледь не в кожній поїздці президента чи прем’єра регіонами.  Якщо демократизація, яка розпочалася з часів горбачовської перебудови, змусила політиків бути більш публічними, то «червоних директорів» ця вимога часу зовсім не торкнулася. Чимало нинішніх керівників  підприємств – індустріальних монстрів часів СРСР – у своїй поведінці нагадують героя серіалу «Чорнобиль» Анатолія Дятлова, інженера ЧАЕС. Зухвалий, хамовитий, не схильний чути будь-яку думку, окрім власної – такий тип керівників виховували на так званих «закритих» підприємствах: оборонних заводах, атомних станціях тощо. Режим секретності й ласка від влади дозволяли поводитися з підлеглими саме так, як Дятлов. У сучасній Україні чимало таких директорів залишилося у південно-східних регіонах. Усі ці «міцні господарники», які на догоду своїм політичним партнерам з Партії регіонів завжди могли організувати кілька автобусів з робітниками – для Антимайдану чи якоїсь подібної акції. Великі металургійні, хімічні, машинобудівні підприємства залишаються потенційними «чорнобилями». Не в останню чергу – через керівників старої формації. Про складну екологічну ситуацію в Маріуполі писали дуже багато разів. Але, як не дивно, серед місцевих, особливо працівників ахметовських меткомбінтаів, найпоширенішим є настрій: «Так, викиди вбивають довкілля, але ж ці заводи дають нам роботу – то чи є сенс щось змінювати»? Люди озвучують позицію керівництва, не сміють йти проти неї. Так само, як молоді співробітники ЧАЕС у серіалі.  Навіть у комерційних фірмах таких некомпетентних самодурів немало. Як свідчать соцопитування конфлікти з керівництвом входить в ТОП-3 причин звільнення звільнення з роботи. Керівники навіть несвідомо копіюють цю радянську модель. По київських редакціях ходить легенда про редактора, який так само кричав і принижував підлеглих, як і Дятлов.   Хамство в розмовах звичне і для першого кабінету країни. Записи директора Кучми всі чули; Ющенко, попри інтелігентний вигляд, дозволяв собі на людях звертатися до незнайомих чиновників на «ти», кричати на них; про  Януковича кажуть, що міг і в морду дати (але доказів немає); за свідченнями оточення Порошенка, він теж досить зверхньо і зневажливо міг говорити зі своїми підлеглими та політичними партнерами. Реального Зеленського ми побачили під час першої спроби Радіо «Свобода» взяти в нього інтерв’ю. Потім цей нахабний і зневажливий стиль спілкування перейшов у відеозвернення.  Що там президенти! Подивіться, як говорять вчителі з дітьми у школах чи батьки на дитячих майданчиках. Таких «дятлових» сотні тисяч, і вони передають свою культуру (в розумінні набору навичок спілкування, звичок, світогляду тощо) наступним поколінням. Мабуть скрізь у світі є люди, які нехтують посадовими інструкціями з безпека. Але в Союзі це увійло в культ. Сцена в останній серії, коли працівник апаратної кричить, що цього робити не можна, але начальник каже \"Робіть!\" західним глядачем сприймається, мабуть, як виняткове самодурство. Та ми втаємничені і знаємо, що нехтування безпекою загальне правило на наших теренах. Крайнім і абсурдним проявом цього культурного феномену є відмова частини водіїв пристібатися пасками безпеки.  Чорнобиль породив не лише недовіру до чиновників, а й до техніки. Вагома частина серіалу присвячена пошуку причин аварії на ЧАЕС, і одна з них – дефект радянських атомних реакторів. Визнати це перед усім світом, з точки зору радянського керівництва, було неприпустимо – і навіть всередині країни говорити про це є зухвалим злочином. В українцях відтоді живе переконаність, що будь-яка техніка вітчизняного виробництва – неякісна і небезпечна для життя. З останніх прикладів – міномети «Молот»: чимало фахівців говорить про те, що фатальні вибухи під час стрільб найчастіше трапляються через помилки розрахунків (подвійне заряджання, наприклад). Проте, дуже популярним є настрій «це українські міномети такі неякісні».  Те саме стосується й цивільних сфер життя: австралійці можуть радісно зустрічати українську «Мрію». Українці ж воліють здебільшого не помічати жодних технологічних розробок на Батьківщині: дається взнаки скепсис, народжений в роки перебудов. Частково він породжений об’єктивно вищою якістю багатьох західних товарів, частково – розчаруваннями в радянському «науково-технічному прогресі», який обернувся страхітливою техногенною аварією. у фільмі присутня суб’єктність колонії Україна Фільмові інколи закидають: Україна в ньому, мовляв, як справжня колонія, позбавлена суб’єктності. Докір не зовсім справедливий. По-перше, станом на 1986-ий рік УРСР дійсно була позбавлена суб’єктності. По-друге, у фільмі присутня суб’єктність колонії. Один із сильних моментів фільму: діалог солдата й бабці, що не збирається залишати свою оселю та згадує усі трагедії, які пережила Україна в ХХ столітті. У цій бабусі легко вгадати нинішніх мешканців прифронтових сіл на Донбасі, які, хай там що, не збираються покидати свої помешкання. Укоріненість – це те, що не змінюється з роками. Війна – єдиний порівнюваний з Чорнобилем шок, який Україна пережила у новітній історії. І тоді, і тепер все значною мірою трималося на людях із розвиненим почуттям обов’язку: вчених, військових, добровольцях. Вони можуть входити у конфлікт із владою, не знати реального масштабу загроз, але виконувати свою справу під десантним гаслом «ніхто крім нас». З одним відчутним «але»: примусу в Україні значно менше, і ризикують життям ті, хто морально готовий для цього.\n"
     ]
    }
   ],
   "source": [
    "example_article = articles.article[1]\n",
    "example_title = articles.title[1]\n",
    "\n",
    "print(\"Title: \" + example_title)\n",
    "print(\"Article: \" + example_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.uk.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C_7BtlGGZ2Af"
   },
   "outputs": [],
   "source": [
    "def clean_text(text, remove_stopwords = True):\n",
    "    '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''\n",
    "    \n",
    "    # Convert words to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Format words and remove unwanted characters\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    pattern = re.compile('[\\W_]+')\n",
    "    text = pattern.sub(' ', text)\n",
    "    \n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = STOP_WORDS\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'влада погано комунікує суспільством чуємо явища причин одна криється чорнобильській трагедії аварію причини наслідки попервах погано розуміли кремлі усіляко приховували применшували власне срср доброю традицією чаес першою техногенною катастрофою варто задати киштимську аварію уралі 1957 го масштаб інформаційний ефект чорнобиля нечуваний відтоді серед українців залишилася помітна травма трапляється надзвичайне влада вочевидь брехатиме викривлятиме інформацію соціумі велике поле чуток маніпуляцій добу соцмереж являються фейкові повідомлення стилі чоловік подруги працює пожежником чорнобильські зоні йдуть різні варіації жахалок заходить 30 вибуху аес надто цинічною брехня брехатимуть впевнені мільйони громадян незалежній україні тотальної цензури всемогутнього кгб імперських амбіцій чиновникам надзвичайних ситуацій доводиться докладати чимало зусиль викликати бодай якусь довіру порівняно недавня історія 2007 го села ожидів львівщині перекинулися цистерни фосфором генерал олександр кузьмук віце прем єром приїхав місце подій заспокоїти місцевих мешканців скуштував огірок городу місцевої мешканки погляд типова ляшковщина пережиток чорнобиля однойменному серіалі промовиста сцена шахтар кидає стіл московськими чиновниками респіратор словами якби респіратори допомагали носили знімаючи чорнобильська брехня породила недовіру представники влади змушені переконувати людей власним емпіричним прикладом бодай персональною присутністю торік масове отруєння дітей черкасах володимир гройсман особисто літав провідати люди молодшого покоління висміяли численних фотожабах покоління яке пережило чорнобиль особиста присутність очільника уряду стала переконливим спростуванням неймовірних чуток екологічну ситуацію місті серіалі доволі промовисто показано ієрархію радянського чиновництва дрібні місцеві функціонери прагнуть обдурити столичних шишок уникнути відповідальності зіграти некомпетентності високих гостей прип ятські можновладці зустрічають бориса щербину впевнені зуміють навіщати локшини вуха кремлівському партійному бонзі їхньому здивуванню меж товариш цк демонструє знання атомної енергетики українськими чиновниками візитерам києва прагнуть продемонструвати показуху пускати фасаду заколисувати їхню увагу замість ділової розмови перевести візит банкет п янку радянські прихвати помітити ледь кожній поїздці президента прем єра регіонами демократизація розпочалася часів горбачовської перебудови змусила політиків публічними червоних директорів вимога торкнулася чимало нинішніх керівників підприємств індустріальних монстрів часів срср поведінці нагадують героя серіалу чорнобиль анатолія дятлова інженера чаес зухвалий хамовитий схильний чути яку думку власної тип керівників виховували званих закритих підприємствах оборонних заводах атомних станціях тощо режим секретності влади дозволяли поводитися підлеглими дятлов сучасній україні чимало таких директорів залишилося південно східних регіонах міцні господарники догоду своїм політичним партнерам партії регіонів могли організувати автобусів робітниками антимайдану якоїсь подібної акції великі металургійні хімічні машинобудівні підприємства залишаються потенційними чорнобилями останню чергу керівників старої формації складну екологічну ситуацію маріуполі писали разів дивно серед місцевих працівників ахметовських меткомбінтаів найпоширенішим настрій викиди вбивають довкілля заводи дають роботу сенс змінювати люди озвучують позицію керівництва сміють йти молоді співробітники чаес серіалі комерційних фірмах таких некомпетентних самодурів немало свідчать соцопитування конфлікти керівництвом входить топ 3 причин звільнення звільнення роботи керівники несвідомо копіюють радянську модель київських редакціях ходить легенда редактора кричав принижував підлеглих дятлов хамство розмовах звичне першого кабінету країни записи директора кучми чули ющенко попри інтелігентний вигляд дозволяв людях звертатися незнайомих чиновників кричати януковича кажуть морду дати доказів свідченнями оточення порошенка зверхньо зневажливо говорити своїми підлеглими політичними партнерами реального зеленського побачили першої спроби радіо свобода взяти інтерв ю нахабний зневажливий стиль спілкування перейшов відеозвернення президенти подивіться говорять вчителі дітьми школах батьки дитячих майданчиках таких дятлових сотні передають культуру розумінні набору навичок спілкування звичок світогляду тощо наступним поколінням мабуть світі люди нехтують посадовими інструкціями безпека союзі увійло культ сцена останній серії працівник апаратної кричить робити начальник робіть західним глядачем сприймається мабуть виняткове самодурство втаємничені знаємо нехтування безпекою загальне правило теренах крайнім абсурдним проявом культурного феномену відмова частини водіїв пристібатися пасками безпеки чорнобиль породив недовіру чиновників техніки вагома частина серіалу присвячена пошуку причин аварії чаес одна дефект радянських атомних реакторів визнати світом точки зору радянського керівництва неприпустимо всередині країни говорити зухвалим злочином українцях відтоді живе переконаність техніка вітчизняного виробництва неякісна небезпечна життя останніх прикладів міномети молот чимало фахівців фатальні вибухи стрільб найчастіше трапляються помилки розрахунків подвійне заряджання наприклад популярним настрій українські міномети неякісні стосується цивільних сфер життя австралійці радісно зустрічати українську мрію українці воліють здебільшого помічати жодних технологічних розробок батьківщині дається взнаки скепсис народжений перебудов частково породжений об єктивно вищою якістю багатьох західних товарів частково розчаруваннями радянському науково технічному прогресі обернувся страхітливою техногенною аварією фільмі присутня суб єктність колонії україна фільмові інколи закидають україна мовляв справжня колонія позбавлена суб єктності докір справедливий перше станом 1986 ий урср позбавлена суб єктності друге фільмі присутня суб єктність колонії сильних моментів фільму діалог солдата бабці збирається залишати оселю згадує трагедії пережила україна хх столітті бабусі легко вгадати нинішніх мешканців прифронтових сіл донбасі хай збираються покидати помешкання укоріненість змінюється роками війна єдиний порівнюваний чорнобилем шок україна пережила новітній історії значною мірою трималося людях розвиненим почуттям обов язку вчених військових добровольцях входити конфлікт владою знати реального масштабу загроз виконувати справу десантним гаслом ніхто одним відчутним примусу україні значно ризикують життям морально готовий'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(example_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "MZg_8goOZ4yy",
    "outputId": "dd020f11-5081-4521-d663-b86ece4e7d12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries are complete.\n",
      "Texts are complete.\n"
     ]
    }
   ],
   "source": [
    "# Clean the summaries and texts\n",
    "clean_summaries = []\n",
    "for summary in articles.title:\n",
    "    clean_summaries.append(clean_text(summary, remove_stopwords=False))\n",
    "print(\"Summaries are complete.\")\n",
    "\n",
    "clean_texts = []\n",
    "for text in articles.article:\n",
    "    clean_texts.append(clean_text(text))\n",
    "print(\"Texts are complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "colab_type": "code",
    "id": "t1MO5QwNZ62b",
    "outputId": "1a8627f0-4dec-4d1b-c555-8a57c453742d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 хвилин на реакцію як ефективно протидіяти дезінформації не запроваджуючи цензуру досвід тайваню\n",
      "\n",
      "одрі танг любить висловлюватися точно інтерв ю тайванський міністр портфеля табличці іменем танга написано цифровий міністр негайно виправляє згадуємо термін фейкові новини прийнятний термін дезінформація танг юридичне поняття тайвані тобто навмисна спрямована шкоду неправда найважливіше шкодить громадськості демократичній системі образу міністра сміхом шкодити міністру звичайна якісна журналістика відміну владних режимів азії таких сингапур тайвань вирішив боротися пошестю дезінформації вдаючись цензури арештів тайвань вийшов воєнного стану 1987 провів перші президентські вибори 1996 му попри економіка тайваню суттєво залежить торгівлі інвестицій китаю вважає тайвань своєю територією демократія продовжувала відокремлювати політичної системи континенті виборці тайваню обирають партією гоміндан схильна тісніших стосунків китаєм демократичною прогресивною партією дпп наголошує автономії прямій незалежності китаю чинна президентка цай інвень належить дпп танг програмістка вийшла хакерського середовища минулого тижня зустрілася журналістом комітету захисту журналістів тайпеї мета поспілкуватися тайвань намагається зберегти чесність демократичність змі противагу своєму значно більшому опоненту китаю жорстко контролює власні медіа потенційну змогу посіяти хаос відкритій медійній системі тайваню вважаю тайвані найбільшою мірою покладаємося власну здатність суспільства виявляти дезінформацію тобто навмисну спрямовану завдання шкоди неправду противагу журналістській роботі легко тридцять тридцять п ять тайвань перебуває китайська народна республіка людей старше покоління мають труднощі розрізненням дезінформації справжньої журналістської роботи державні змі єдиними змі відверто кажучи чимало пропаганди тож відмінності побачити людей народилися здобували освіту скасування воєнного стану тобто 80 х широкий діапазон інформаційних джерел їхній вибір демократія розпочалася перших президентських виборів 1996 му збіглася часі появою world wide web тож люди пов язують демократію демократизацією джерел інформації дезінформація небезпека відкритих суспільств тайваню режимів кнр використовують дезінформацію привід держави запровадити цензуру хочемо йти шляхом пам ятаємо воєнний стан перше масового поширення пропагандистської дезінформаційної кампанії спостерігаємо появу відправної точки здійснюють певне тестування перевірку різних варіантів стануть справді популярними тестуються меми різні версії побачити отримають вірусне поширення міністерств команду відповідальну разі виявлення дезінформаційної кампанії сягнула мас протягом 60 хвилин виготовити переконливе повідомлення короткий фільм картка медіа допис соціальних мережах міністр робить стрім президент прийшла комедійне шоу прем єр міністр дивиться стрім відеогри виявили робимо більшість населення отримує повідомлення щеплення дійде дезінформація тож захист працює подібно вакцини ведемо зразок таблиці обліку визначаємо міністерству реагування міністерству внутрішніх середньому 60 хвилин міністерству здоров добробуту середньому 70 змагаються своєрідній дружній конкуренції реагують швидше швидше вийдуть 60 хвилинну межу передають меседжі платформу миттєвих повідомлень line facebook екаунти президента цай інвень прем єра віце прем єра кожний велику кількість підписників суті просять фоловерів поширити роз яснення надійшло їхніх друзів родичів повідомлення дезінформацією традиційні змі звісно отримують контрповідомлення завдяки готують збалансовані публікації побачили випадку запустили контрповідомлення приготували відеоролики фільми картинки годин починається новий цикл новин змі протидія безнадійна чесно кажучи насправді виснажує деякі стають вірусними непомітно відбувається каналах застосовують наскрізне шифрування відміну фейсбук твіттер систем працює індексація гугл канали недоступні пошукових механізмів закрита кімната легко провести мутацію дезінформації потужний мем перш випустять мовити світ широкий розробили співпраці соціальною мережею line facebook систему яку називаємо сповіщення громадське сповіщення система працює подібно спам фільтра отримали електронний лист думаєте спам сміттєва реклама теоретично приватні комунікації держава переглядати пошту думаєте лист надійшов якоїсь країни принцеса хоче поділитися п ятьма мільйонами доларів можете позначити лист спам двохтисячних інтернет спільнота переконали операторів електронної пошти додати таку кнопку інтерфейсу ставили помітку спам пересилали текст повідомлення обов язково добровільне пожертвування глобальній системі назвою spamhaus список блокування доменів тощо існує ціла система імунна система електронної пошти значна кількість людей ставить прапорець спам робить кореляцію відправником листа відправляє наступного листа доходить адресата цензури нема потрапляє папки сміттєвою поштою замовчанням відбирати людей розробляємо схожу систему люди пересилати онлайнову інформацію повідомлення отримані системах негайних повідомлень спеціальному боту наразі популярний бот називається cofact тобто collaborative fact колективний факт скоро червні соціальна система line вбудує функціональність зробити натиснути повідомлення можете позначити дезінформацію наприклад справді популярна чутка випадку землетрусу потужнішого балів сусідні держави надсилати команди рятувальників згоди країни постраждала землетрусу привід окупації розумієте бог знає поширювали повідомлення якому випадку популярне повідомлення тож тайванський центр фактчекінгу досліджує міжнародні угоди договори підписані міністром закордонних подібні речі цитати джерела цитат зрештою заявляє повідомлення фальшивкою зробили facebook пообіцяв червня уточнять алгоритм facebook припинив поширювати повідомлення популярне стрічці цензурою подивитеся стрічку друга повідомлення залишилося містить попередження перевірка фактів встановила неправдивість напрацьовуємо схожі домовленості іншми соціальними мережами line популярна азії система обміну миттєвими повідомленнями прим facebook долучилися впровадження системи сповіщення громадське сповіщення ніяк системою сповіщення вилучення інформації примітка редактора речник line повідомив телефоном cpj співпрацюють групами факт чекінгу такими cofact ｍygopen taiwan factcheck center rumor truth задля створення офіційного екаунту line перевірки фактів користувачі зможуть пересилати сумнівну інформацію робити запит перевірку відповідності фактам facebook відповів негайно запит електронною поштою коментар виборів діє спеціальний набір правил обмежує пожертви кампанію відповідно відзначили іноземні кошти надходять шляхом виявлено надають перевагу закупівлі точно націленої реклами соціальних мережах традиційних змі тож кажемо пожертви кампанію слід розкрити інформацію дані пожертви місцеве населення право витрачати гроші фінансувати політичну рекламу рекламне агентство посередник повинен повідомити звідки надійшли кошти боротьба відмиванням грошей кінці ланцюжка виявиться іноземець кнр макао гонконг насправді злочином вибори особливою ситуацією захищаємо виборчий процес значно вищому рівіні звичайна система повідомлень громадських повідомлень\n"
     ]
    }
   ],
   "source": [
    "# Example of cleaned article\n",
    "print(clean_summaries[0])\n",
    "print()\n",
    "print(clean_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count the number of occurrences of each word in a set of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aKm8RMdya1Rh"
   },
   "outputs": [],
   "source": [
    "def count_words(count_dict, text):\n",
    "    '''Count the number of occurrences of each word in a set of text'''\n",
    "    for sentence in text:\n",
    "        for word in sentence.split():\n",
    "            if word not in count_dict:\n",
    "                count_dict[word] = 1\n",
    "            else:\n",
    "                count_dict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ми': 1,\n",
       " 'будемо': 1,\n",
       " 'їсти': 2,\n",
       " 'піццу': 1,\n",
       " 'сьогодні': 1,\n",
       " 'ввечері': 1,\n",
       " 'овочі': 1,\n",
       " 'корисно': 1}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydict = {}\n",
    "count_words(mydict, [\"ми будемо їсти піццу сьогодні ввечері\", \"їсти овочі корисно\"])\n",
    "mydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "RYMNdxR-a3R4",
    "outputId": "4750104a-278a-48aa-f227-2f0504ea1070"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocabulary: 72434\n"
     ]
    }
   ],
   "source": [
    "word_counts = {}\n",
    "\n",
    "count_words(word_counts, clean_summaries)\n",
    "count_words(word_counts, clean_texts)\n",
    "            \n",
    "print(\"Size of Vocabulary:\", len(word_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load embeddings (TODO: )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Es_I5U0xa5S5",
    "outputId": "4e373d1c-da1f-4d13-cd39-9fe9dad89c4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings: 484557\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open('/content/gdrive/My Drive/ML summarization/deeplearning/numberbatch-en-17.02.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        embedding = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = embedding\n",
    "\n",
    "print('Word embeddings:', len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "AW802uiNa75r",
    "outputId": "1671d467-611c-49c2-b660-c0d30f0ff431"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words missing from CN: 3044\n",
      "Percent of words that are missing from vocabulary: 2.29%\n"
     ]
    }
   ],
   "source": [
    "missing_words = 0\n",
    "threshold = 20\n",
    "\n",
    "for word, count in word_counts.items():\n",
    "    if count > threshold:\n",
    "        if word not in embeddings_index:\n",
    "            missing_words += 1\n",
    "            \n",
    "missing_ratio = round(missing_words/len(word_counts),4)*100\n",
    "            \n",
    "print(\"Number of words missing from CN:\", missing_words)\n",
    "print(\"Percent of words that are missing from vocabulary: {}%\".format(missing_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "Kmqng2XHh6_p",
    "outputId": "1db59845-6d51-4c37-f499-460c7ce4a080"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words: 132884\n",
      "Number of words we will use: 65469\n",
      "Percent of words we will use: 49.27%\n"
     ]
    }
   ],
   "source": [
    "vocab_to_int = {} \n",
    "\n",
    "value = 0\n",
    "for word, count in word_counts.items():\n",
    "    if count >= threshold or word in embeddings_index:\n",
    "        vocab_to_int[word] = value\n",
    "        value += 1\n",
    "\n",
    "# Special tokens that will be added to our vocab\n",
    "codes = [\"<UNK>\",\"<PAD>\",\"<EOS>\",\"<GO>\"]   \n",
    "\n",
    "# Add codes to vocab\n",
    "for code in codes:\n",
    "    vocab_to_int[code] = len(vocab_to_int)\n",
    "\n",
    "# Dictionary to convert integers to words\n",
    "int_to_vocab = {}\n",
    "for word, value in vocab_to_int.items():\n",
    "    int_to_vocab[value] = word\n",
    "\n",
    "usage_ratio = round(len(vocab_to_int) / len(word_counts),4)*100\n",
    "\n",
    "print(\"Total number of unique words:\", len(word_counts))\n",
    "print(\"Number of words we will use:\", len(vocab_to_int))\n",
    "print(\"Percent of words we will use: {}%\".format(usage_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "rwT6WIjDh-5I",
    "outputId": "7f53a08f-8158-4681-996b-e1bdd7a2322b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65469\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 300\n",
    "nb_words = len(vocab_to_int)\n",
    "\n",
    "# Create matrix with default values of zero\n",
    "word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\n",
    "for word, i in vocab_to_int.items():\n",
    "    if word in embeddings_index:\n",
    "        word_embedding_matrix[i] = embeddings_index[word]\n",
    "    else:\n",
    "        # If word not in CN, create a random embedding for it\n",
    "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "        embeddings_index[word] = new_embedding\n",
    "        word_embedding_matrix[i] = new_embedding\n",
    "\n",
    "# Check if value matches len(vocab_to_int)\n",
    "print(len(word_embedding_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2gobgfFZiCHR"
   },
   "outputs": [],
   "source": [
    "def convert_to_ints(text, word_count, unk_count, eos=False):\n",
    "    '''Convert words in text to an integer.\n",
    "       If word is not in vocab_to_int, use UNK's integer.\n",
    "       Total the number of words and UNKs.\n",
    "       Add EOS token to the end of texts'''\n",
    "    ints = []\n",
    "    for sentence in text:\n",
    "        sentence_ints = []\n",
    "        for word in sentence.split():\n",
    "            word_count += 1\n",
    "            if word in vocab_to_int:\n",
    "                sentence_ints.append(vocab_to_int[word])\n",
    "            else:\n",
    "                sentence_ints.append(vocab_to_int[\"<UNK>\"])\n",
    "                unk_count += 1\n",
    "        if eos:\n",
    "            sentence_ints.append(vocab_to_int[\"<EOS>\"])\n",
    "        ints.append(sentence_ints)\n",
    "    return ints, word_count, unk_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "uHOLyENmiFmo",
    "outputId": "cb410300-aff9-4ea9-a07f-746a2dc5a8b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in headlines: 25679933\n",
      "Total number of UNKs in headlines: 170450\n",
      "Percent of words that are UNK: 0.66%\n"
     ]
    }
   ],
   "source": [
    "word_count = 0\n",
    "unk_count = 0\n",
    "\n",
    "int_summaries, word_count, unk_count = convert_to_ints(clean_summaries, word_count, unk_count)\n",
    "int_texts, word_count, unk_count = convert_to_ints(clean_texts, word_count, unk_count, eos=True)\n",
    "\n",
    "unk_percent = round(unk_count/word_count,4)*100\n",
    "\n",
    "print(\"Total number of words in headlines:\", word_count)\n",
    "print(\"Total number of UNKs in headlines:\", unk_count)\n",
    "print(\"Percent of words that are UNK: {}%\".format(unk_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2OD95WMaiIVK"
   },
   "outputs": [],
   "source": [
    "def create_lengths(text):\n",
    "    '''Create a data frame of the sentence lengths from a text'''\n",
    "    lengths = []\n",
    "    for sentence in text:\n",
    "        lengths.append(len(sentence))\n",
    "    return pd.DataFrame(lengths, columns=['counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "colab_type": "code",
    "id": "IBDnDWjfiMXu",
    "outputId": "290137d6-0375-42d0-92f0-1234860d73ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries:\n",
      "              counts\n",
      "count  568411.000000\n",
      "mean        4.181624\n",
      "std         2.657872\n",
      "min         0.000000\n",
      "25%         2.000000\n",
      "50%         4.000000\n",
      "75%         5.000000\n",
      "max        48.000000\n",
      "\n",
      "Texts:\n",
      "              counts\n",
      "count  568411.000000\n",
      "mean       41.996835\n",
      "std        42.520873\n",
      "min         1.000000\n",
      "25%        18.000000\n",
      "50%        29.000000\n",
      "75%        50.000000\n",
      "max      2085.000000\n"
     ]
    }
   ],
   "source": [
    "lengths_summaries = create_lengths(int_summaries)\n",
    "lengths_texts = create_lengths(int_texts)\n",
    "\n",
    "print(\"Summaries:\")\n",
    "print(lengths_summaries.describe())\n",
    "print()\n",
    "print(\"Texts:\")\n",
    "print(lengths_texts.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9ftG9iqtiRhC"
   },
   "outputs": [],
   "source": [
    "def unk_counter(sentence):\n",
    "    '''Counts the number of time UNK appears in a sentence.'''\n",
    "    unk_count = 0\n",
    "    for word in sentence:\n",
    "        if word == vocab_to_int[\"<UNK>\"]:\n",
    "            unk_count += 1\n",
    "    return unk_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "T_zClxvniYDy",
    "outputId": "e983e746-42ad-458f-bec6-bdea2b43258f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429209\n",
      "429209\n"
     ]
    }
   ],
   "source": [
    "sorted_summaries = []\n",
    "sorted_texts = []\n",
    "max_text_length = 84\n",
    "max_summary_length = 13\n",
    "min_length = 2\n",
    "unk_text_limit = 1\n",
    "unk_summary_limit = 0\n",
    "\n",
    "for length in range(min(lengths_texts.counts), max_text_length): \n",
    "    for count, words in enumerate(int_summaries):\n",
    "        if (len(int_summaries[count]) >= min_length and\n",
    "            len(int_summaries[count]) <= max_summary_length and\n",
    "            len(int_texts[count]) >= min_length and\n",
    "            unk_counter(int_summaries[count]) <= unk_summary_limit and\n",
    "            unk_counter(int_texts[count]) <= unk_text_limit and\n",
    "            length == len(int_texts[count])\n",
    "           ):\n",
    "            sorted_summaries.append(int_summaries[count])\n",
    "            sorted_texts.append(int_texts[count])\n",
    "        \n",
    "# Compare lengths to ensure they match\n",
    "print(len(sorted_summaries))\n",
    "print(len(sorted_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-JU3DbUGibgt"
   },
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    '''Create palceholders for inputs to the model'''\n",
    "    \n",
    "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    summary_length = tf.placeholder(tf.int32, (None,), name='summary_length')\n",
    "    max_summary_length = tf.reduce_max(summary_length, name='max_dec_len')\n",
    "    text_length = tf.placeholder(tf.int32, (None,), name='text_length')\n",
    "\n",
    "    return input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E75uw3LQirgX"
   },
   "outputs": [],
   "source": [
    "def process_encoding_input(target_data, vocab_to_int, batch_size):\n",
    "    '''Remove the last word id from each batch and concat the <GO> to the begining of each batch'''\n",
    "    \n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
    "\n",
    "    return dec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RRmHqVSCiujL"
   },
   "outputs": [],
   "source": [
    "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob):\n",
    "    '''Create the encoding layer'''\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        with tf.variable_scope('encoder_{}'.format(layer)):\n",
    "            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \n",
    "                                                    input_keep_prob = keep_prob)\n",
    "\n",
    "            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, \n",
    "                                                    input_keep_prob = keep_prob)\n",
    "\n",
    "            enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n",
    "                                                                    cell_bw, \n",
    "                                                                    rnn_inputs,\n",
    "                                                                    sequence_length,\n",
    "                                                                    dtype=tf.float32)\n",
    "    # Join outputs since we are using a bidirectional RNN\n",
    "    enc_output = tf.concat(enc_output,2)\n",
    "    \n",
    "    return enc_output, enc_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o312UorKiyhE"
   },
   "outputs": [],
   "source": [
    "def training_decoding_layer(dec_embed_input, summary_length, dec_cell, initial_state, output_layer, \n",
    "                            vocab_size, max_summary_length):\n",
    "    '''Create the training logits'''\n",
    "    \n",
    "    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
    "                                                        sequence_length=summary_length,\n",
    "                                                        time_major=False)\n",
    "\n",
    "    training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                       training_helper,\n",
    "                                                       initial_state,\n",
    "                                                       output_layer) \n",
    "\n",
    "    training_logits, _ , _ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                                           output_time_major=False,\n",
    "                                                           impute_finished=True,\n",
    "                                                           maximum_iterations=max_summary_length)\n",
    "    return training_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lJfDRX6yi1dD"
   },
   "outputs": [],
   "source": [
    "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, initial_state, output_layer,\n",
    "                             max_summary_length, batch_size):\n",
    "    '''Create the inference logits'''\n",
    "    \n",
    "    start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "    \n",
    "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
    "                                                                start_tokens,\n",
    "                                                                end_token)\n",
    "                \n",
    "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                        inference_helper,\n",
    "                                                        initial_state,\n",
    "                                                        output_layer)\n",
    "                \n",
    "    inference_logits, _ , _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                                            output_time_major=False,\n",
    "                                                            impute_finished=True,\n",
    "                                                            maximum_iterations=max_summary_length)\n",
    "    \n",
    "    return inference_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bPQ4wxdyi37i"
   },
   "outputs": [],
   "source": [
    "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, text_length, summary_length, \n",
    "                   max_summary_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers):\n",
    "    '''Create the decoding cell and attention for the training and inference decoding layers'''\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        with tf.variable_scope('decoder_{}'.format(layer)):\n",
    "            lstm = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                           initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            dec_cell = tf.contrib.rnn.DropoutWrapper(lstm, \n",
    "                                                     input_keep_prob = keep_prob)\n",
    "    \n",
    "    output_layer = Dense(vocab_size,\n",
    "                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
    "    \n",
    "    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n",
    "                                                  enc_output,\n",
    "                                                  text_length,\n",
    "                                                  normalize=False,\n",
    "                                                  name='BahdanauAttention')\n",
    "\n",
    "    dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell,\n",
    "                                                          attn_mech,\n",
    "                                                          rnn_size)\n",
    "            \n",
    "    #initial_state = tf.contrib.seq2seq.AttentionWrapperState(enc_state[0],\n",
    "    #                                                                _zero_state_tensors(rnn_size, \n",
    "    #                                                                                    batch_size, \n",
    "    #                                                                                    tf.float32)) \n",
    "    initial_state = dec_cell.zero_state(batch_size=batch_size,dtype=tf.float32).clone(cell_state=enc_state[0])\n",
    "\n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        training_decoder = training_decoding_layer(dec_embed_input, \n",
    "                                                  summary_length, \n",
    "                                                  dec_cell, \n",
    "                                                  initial_state,\n",
    "                                                  output_layer,\n",
    "                                                  vocab_size, \n",
    "                                                  max_summary_length)\n",
    "        \n",
    "        training_logits,_ ,_ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                  output_time_major=False,\n",
    "                                  impute_finished=True,\n",
    "                                  maximum_iterations=max_summary_length)\n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        inference_decoder = inference_decoding_layer(embeddings,  \n",
    "                                                    vocab_to_int['<GO>'], \n",
    "                                                    vocab_to_int['<EOS>'],\n",
    "                                                    dec_cell, \n",
    "                                                    initial_state, \n",
    "                                                    output_layer,\n",
    "                                                    max_summary_length,\n",
    "                                                    batch_size)\n",
    "        \n",
    "        inference_logits,_ ,_ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                  output_time_major=False,\n",
    "                                  impute_finished=True,\n",
    "                                  maximum_iterations=max_summary_length)\n",
    "\n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "orftVP-AjqNF"
   },
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length, \n",
    "                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size):\n",
    "    '''Use the previous functions to create the training and inference logits'''\n",
    "    \n",
    "    # Use Numberbatch's embeddings and the newly created ones as our embeddings\n",
    "    embeddings = word_embedding_matrix\n",
    "    \n",
    "    enc_embed_input = tf.nn.embedding_lookup(embeddings, input_data)\n",
    "    enc_output, enc_state = encoding_layer(rnn_size, text_length, num_layers, enc_embed_input, keep_prob)\n",
    "    \n",
    "    dec_input = process_encoding_input(target_data, vocab_to_int, batch_size)\n",
    "    dec_embed_input = tf.nn.embedding_lookup(embeddings, dec_input)\n",
    "    \n",
    "    training_logits, inference_logits  = decoding_layer(dec_embed_input, \n",
    "                                                        embeddings,\n",
    "                                                        enc_output,\n",
    "                                                        enc_state, \n",
    "                                                        vocab_size, \n",
    "                                                        text_length, \n",
    "                                                        summary_length, \n",
    "                                                        max_summary_length,\n",
    "                                                        rnn_size, \n",
    "                                                        vocab_to_int, \n",
    "                                                        keep_prob, \n",
    "                                                        batch_size,\n",
    "                                                        num_layers)\n",
    "    \n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xXDlzjChjwan"
   },
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch):\n",
    "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6HHWN1-Ojyzd"
   },
   "outputs": [],
   "source": [
    "def get_batches(summaries, texts, batch_size):\n",
    "    \"\"\"Batch summaries, texts, and the lengths of their sentences together\"\"\"\n",
    "    for batch_i in range(0, len(texts)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        summaries_batch = summaries[start_i:start_i + batch_size]\n",
    "        texts_batch = texts[start_i:start_i + batch_size]\n",
    "        pad_summaries_batch = np.array(pad_sentence_batch(summaries_batch))\n",
    "        pad_texts_batch = np.array(pad_sentence_batch(texts_batch))\n",
    "        \n",
    "        # Need the lengths for the _lengths parameters\n",
    "        pad_summaries_lengths = []\n",
    "        for summary in pad_summaries_batch:\n",
    "            pad_summaries_lengths.append(len(summary))\n",
    "        \n",
    "        pad_texts_lengths = []\n",
    "        for text in pad_texts_batch:\n",
    "            pad_texts_lengths.append(len(text))\n",
    "        \n",
    "        yield pad_summaries_batch, pad_texts_batch, pad_summaries_lengths, pad_texts_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NGYG2IOfj10V"
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 64\n",
    "rnn_size = 256\n",
    "num_layers = 2\n",
    "learning_rate = 0.005\n",
    "keep_probability = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "colab_type": "code",
    "id": "uxuqzSkzj6yG",
    "outputId": "8c047a66-6d54-47d6-f164-6bb5fc097e2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/embedding_ops.py:132: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-33-9390cb26606a>:7: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-33-9390cb26606a>:20: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Graph is built.\n"
     ]
    }
   ],
   "source": [
    "# Build the graph\n",
    "train_graph = tf.Graph()\n",
    "# Set the graph to default to ensure that it is ready for training\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    # Load the model inputs    \n",
    "    input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length = model_inputs()\n",
    "\n",
    "    # Create the training and inference logits\n",
    "    training_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                      targets, \n",
    "                                                      keep_prob,   \n",
    "                                                      text_length,\n",
    "                                                      summary_length,\n",
    "                                                      max_summary_length,\n",
    "                                                      len(vocab_to_int)+1,\n",
    "                                                      rnn_size, \n",
    "                                                      num_layers, \n",
    "                                                      vocab_to_int,\n",
    "                                                      batch_size)\n",
    "    \n",
    "    # Create tensors for the training logits and inference logits\n",
    "    training_logits = tf.identity(training_logits.rnn_output, 'logits')\n",
    "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "    \n",
    "    # Create the weights for sequence_loss\n",
    "    masks = tf.sequence_mask(summary_length, max_summary_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            targets,\n",
    "            masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n",
    "print(\"Graph is built.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "iSkOPDO3kADU",
    "outputId": "2ea26a29-92e0-434a-bce7-18a8d053d896"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shortest text length: 25\n",
      "The longest text length: 83\n"
     ]
    }
   ],
   "source": [
    "start = 200000\n",
    "end = start + 300000\n",
    "sorted_summaries_short = sorted_summaries[start:end]\n",
    "sorted_texts_short = sorted_texts[start:end]\n",
    "print(\"The shortest text length:\", len(sorted_texts_short[0]))\n",
    "print(\"The longest text length:\",len(sorted_texts_short[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KRpUma5TkEdI"
   },
   "outputs": [],
   "source": [
    "learning_rate_decay = 0.95\n",
    "min_learning_rate = 0.0005\n",
    "display_step = 20 # Check training loss after every 20 batches\n",
    "stop_early = 0 \n",
    "stop = 6 #3 # If the update loss does not decrease in 3 consecutive update checks, stop training\n",
    "per_epoch = 3 # Make 3 update checks per epoch\n",
    "update_check = (len(sorted_texts_short)//batch_size//per_epoch)-1\n",
    "\n",
    "update_loss = 0 \n",
    "batch_loss = 0\n",
    "summary_update_loss = [] # Record the update losses for saving improvements in the model\n",
    "\n",
    "  \n",
    "tf.reset_default_graph()\n",
    "checkpoint = \"drive/Colab Notebooks/Menu/Model 300/best_model.ckpt\"  #300k sentence\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # If we want to continue training a previous session\n",
    "    # loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "    # loader.restore(sess, checkpoint)\n",
    "    #sess.run(tf.local_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(1, epochs+1):\n",
    "        update_loss = 0\n",
    "        batch_loss = 0\n",
    "        for batch_i, (summaries_batch, texts_batch, summaries_lengths, texts_lengths) in enumerate(\n",
    "                get_batches(sorted_summaries_short, sorted_texts_short, batch_size)):\n",
    "            start_time = time.time()\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: texts_batch,\n",
    "                 targets: summaries_batch,\n",
    "                 lr: learning_rate,\n",
    "                 summary_length: summaries_lengths,\n",
    "                 text_length: texts_lengths,\n",
    "                 keep_prob: keep_probability})\n",
    "\n",
    "            batch_loss += loss\n",
    "            update_loss += loss\n",
    "            end_time = time.time()\n",
    "            batch_time = end_time - start_time\n",
    "\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                      .format(epoch_i,\n",
    "                              epochs, \n",
    "                              batch_i, \n",
    "                              len(sorted_texts_short) // batch_size, \n",
    "                              batch_loss / display_step, \n",
    "                              batch_time*display_step))\n",
    "                batch_loss = 0\n",
    "                \n",
    "                #saver = tf.train.Saver() \n",
    "                #saver.save(sess, checkpoint)\n",
    "                \n",
    "            if batch_i % update_check == 0 and batch_i > 0:\n",
    "                print(\"Average loss for this update:\", round(update_loss/update_check,3))\n",
    "                summary_update_loss.append(update_loss)\n",
    "                \n",
    "              \n",
    "                  \n",
    "                # If the update loss is at a new minimum, save the model\n",
    "                if update_loss <= min(summary_update_loss):\n",
    "                    print('New Record!') \n",
    "                    stop_early = 0\n",
    "                    saver = tf.train.Saver() \n",
    "                    saver.save(sess, checkpoint)\n",
    "\n",
    "                else:\n",
    "                    print(\"No Improvement.\")\n",
    "                    stop_early += 1\n",
    "                    if stop_early == stop:\n",
    "                        break\n",
    "                update_loss = 0\n",
    "            \n",
    "                    \n",
    "        # Reduce learning rate, but not below its minimum value\n",
    "        learning_rate *= learning_rate_decay\n",
    "        if learning_rate < min_learning_rate:\n",
    "            learning_rate = min_learning_rate\n",
    "        \n",
    "        if stop_early == stop:\n",
    "            print(\"Stopping Training.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6lwZqK_hkKss"
   },
   "outputs": [],
   "source": [
    "checkpoint = \"/content/gdrive/My Drive/ML summarization/deeplearning/best_model.ckpt\" \n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "    loader.restore(sess, checkpoint)\n",
    "    names = []\n",
    "    [names.append(n.name) for n in loaded_graph.as_graph_def().node]\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d6abG-TxkW78"
   },
   "outputs": [],
   "source": [
    "def text_to_seq(text):\n",
    "    '''Prepare the text for the model'''\n",
    "    \n",
    "    text = clean_text(text)\n",
    "    return [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yUEenOMrlLT2"
   },
   "outputs": [],
   "source": [
    "# Create your own review or use one from the dataset\n",
    "#input_sentence = \"I have never eaten an apple before, but this red one was nice. \\\n",
    "                  #I think that I will try a green apple next time.\"\n",
    "#text = text_to_seq(input_sentence)\n",
    "random = np.random.randint(0,len(clean_texts))\n",
    "input_sentence = clean_texts[random]\n",
    "text = text_to_seq(clean_texts[random])\n",
    "\n",
    "checkpoint = \"drive/Colab Notebooks/Menu/Model 300/best_model.ckpt\"\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "    loader.restore(sess, checkpoint)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
    "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "    \n",
    "    #Multiply by batch_size to match the model's input parameters\n",
    "    answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
    "                                      summary_length: [np.random.randint(5,8)], \n",
    "                                      text_length: [len(text)]*batch_size,\n",
    "                                      keep_prob: 1.0})[0] \n",
    "\n",
    "# Remove the padding from the tweet\n",
    "pad = vocab_to_int[\"<PAD>\"] \n",
    "\n",
    "print('Original Text:', reviews.Text[random])\n",
    "print('Original summary:', reviews.Summary[random])#clean_summaries[random]\n",
    "\n",
    "print('\\nText')\n",
    "print('  Word Ids:    {}'.format([i for i in text]))\n",
    "print('  Input Words: {}'.format(\" \".join([int_to_vocab[i] for i in text])))\n",
    "\n",
    "print('\\nSummary')\n",
    "print('  Word Ids:       {}'.format([i for i in answer_logits if i != pad]))\n",
    "print('  Response Words: {}'.format(\" \".join([int_to_vocab[i] for i in answer_logits if i != pad])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h5zINNiDlQXZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "rnnlstm.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
