{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rnnlstm.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SqzyV06rYiLm",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import time\n",
        "from tensorflow.python.layers.core import Dense\n",
        "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7to8sI7iKm-D",
        "colab_type": "text"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQr4VkVEKm-E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "160e95f3-327f-4ef1-9061-967c85ac57c7"
      },
      "source": [
        "articles = pd.read_csv(\"../data/texty_news.csv\", sep='\\t')\n",
        "articles.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>article</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>60 хвилин на реакцію. Як ефективно протидіяти ...</td>\n",
              "      <td>Одрі Танг любить висловлюватися точно. Під час...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Фільм про нас. Cеріал «Чорнобиль» показує вади...</td>\n",
              "      <td>«Влада погано комунікує з суспільством», – чує...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Київські візерунки. Зіграйте у гру й перевірте...</td>\n",
              "      <td>Що бачить художник, коли дивиться на карту Киє...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Протистояння медіа та соцмереж під час виборів...</td>\n",
              "      <td>Модераторка дискусії Діана Дуцик з Могилянсько...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Філарет проти канонічності. Як українські прав...</td>\n",
              "      <td>Попри це, патріархові Філарету потрібно віддат...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title                                            article\n",
              "0  60 хвилин на реакцію. Як ефективно протидіяти ...  Одрі Танг любить висловлюватися точно. Під час...\n",
              "1  Фільм про нас. Cеріал «Чорнобиль» показує вади...  «Влада погано комунікує з суспільством», – чує...\n",
              "2  Київські візерунки. Зіграйте у гру й перевірте...  Що бачить художник, коли дивиться на карту Киє...\n",
              "3  Протистояння медіа та соцмереж під час виборів...  Модераторка дискусії Діана Дуцик з Могилянсько...\n",
              "4  Філарет проти канонічності. Як українські прав...  Попри це, патріархові Філарету потрібно віддат..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpUZr_4DKm-M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "caa2af54-8f16-41e1-8fea-d90110a04d98"
      },
      "source": [
        "articles.isnull().sum()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "title        0\n",
              "article    219\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nz87Wu9rKm-R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "articles = articles.dropna()\n",
        "articles = articles.reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqQcJ3UrKm-U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "c1f6809b-2079-450e-9f7e-4483283b65b7"
      },
      "source": [
        "articles.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>article</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>60 хвилин на реакцію. Як ефективно протидіяти ...</td>\n",
              "      <td>Одрі Танг любить висловлюватися точно. Під час...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Фільм про нас. Cеріал «Чорнобиль» показує вади...</td>\n",
              "      <td>«Влада погано комунікує з суспільством», – чує...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Київські візерунки. Зіграйте у гру й перевірте...</td>\n",
              "      <td>Що бачить художник, коли дивиться на карту Киє...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Протистояння медіа та соцмереж під час виборів...</td>\n",
              "      <td>Модераторка дискусії Діана Дуцик з Могилянсько...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Філарет проти канонічності. Як українські прав...</td>\n",
              "      <td>Попри це, патріархові Філарету потрібно віддат...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title                                            article\n",
              "0  60 хвилин на реакцію. Як ефективно протидіяти ...  Одрі Танг любить висловлюватися точно. Під час...\n",
              "1  Фільм про нас. Cеріал «Чорнобиль» показує вади...  «Влада погано комунікує з суспільством», – чує...\n",
              "2  Київські візерунки. Зіграйте у гру й перевірте...  Що бачить художник, коли дивиться на карту Киє...\n",
              "3  Протистояння медіа та соцмереж під час виборів...  Модераторка дискусії Діана Дуцик з Могилянсько...\n",
              "4  Філарет проти канонічності. Як українські прав...  Попри це, патріархові Філарету потрібно віддат..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zp1vB1sNKm-X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "cc2582df-7e75-4978-8097-40b130bdf28f"
      },
      "source": [
        "example_article = articles.article[1]\n",
        "example_title = articles.title[1]\n",
        "\n",
        "print(\"Title: \" + example_title)\n",
        "print(\"Article: \" + example_article)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Title: Фільм про нас. Cеріал «Чорнобиль» показує вади радянської системи, дотепер вкорінені в Україні\n",
            "Article: «Влада погано комунікує з суспільством», – чуємо ми сьогодні. У цього явища багато причин, і одна з них криється у Чорнобильській трагедії. Аварію, причини й наслідки якої попервах погано розуміли навіть у Кремлі, усіляко приховували, применшували. Власне, для СРСР це було «доброю традицією». ЧАЕС не була першою техногенною катастрофою (варто задати хоча б Киштимську аварію на Уралі 1957-го).  Однак масштаб та інформаційний ефект Чорнобиля – нечуваний. Відтоді серед українців залишилася помітна травма: коли трапляється щось надзвичайне, влада вочевидь брехатиме і викривлятиме інформацію. Тому в нашому соціумі є дуже велике поле для чуток і маніпуляцій.  Вже в добу соцмереж нерідко з’являються фейкові повідомлення в стилі «чоловік моєї подруги працює пожежником у Чорнобильські зоні, і він сказав»… – а далі йдуть різні варіації жахалок. Це «заходить» і через 30 років після вибуху на АЕС, бо надто вже цинічною була брехня тоді.  «Брехатимуть і далі», – впевнені мільйони наших громадян і сьогодні. В незалежній Україні немає тотальної цензури, всемогутнього КГБ, імперських амбіцій. Однак чиновникам під час надзвичайних ситуацій доводиться докладати чимало зусиль, щоб викликати бодай якусь довіру до себе. Порівняно недавня історія: 2007-го року біля села Ожидів на Львівщині перекинулися цистерни з фосфором. Генерал Олександр Кузьмук, який був на той час віце-прем’єром, приїхав на місце подій і, щоб заспокоїти місцевих мешканців, скуштував огірок з городу місцевої мешканки.  На перший погляд, типова «ляшковщина». Але й пережиток Чорнобиля також. В однойменному серіалі є промовиста сцена, коли шахтар кидає на стіл перед московськими чиновниками респіратор зі словами: «Якби вони(респіратори) допомагали, ви б самі носили їх, не знімаючи».  Чорнобильська брехня породила недовіру, коли представники влади змушені переконувати людей власним емпіричним прикладом чи бодай персональною присутністю. Торік було масове отруєння дітей в Черкасах, Володимир Гройсман особисто літав їх провідати. Люди молодшого покоління висміяли це у численних фотожабах. А для покоління, яке пережило Чорнобиль, особиста присутність очільника уряду стала переконливим спростуванням неймовірних чуток про екологічну ситуацію в місті.  В серіалі доволі промовисто показано ієрархію радянського чиновництва: дрібні місцеві функціонери прагнуть обдурити столичних «шишок», уникнути відповідальності, зіграти на некомпетентності високих гостей. Коли прип’ятські можновладці зустрічають Бориса Щербину, то впевнені, що зуміють навіщати локшини на вуха цьому кремлівському партійному бонзі.  Їхньому здивуванню немає меж, коли «товариш із ЦК» демонструє їм знання з атомної енергетики. Так нерідко буває й з українськими чиновниками: візитерам із Києва прагнуть продемонструвати показуху, не пускати «далі фасаду», заколисувати їхню увагу, замість ділової розмови перевести візит у банкет і п’янку. Усі ці радянські прихвати можна помітити ледь не в кожній поїздці президента чи прем’єра регіонами.  Якщо демократизація, яка розпочалася з часів горбачовської перебудови, змусила політиків бути більш публічними, то «червоних директорів» ця вимога часу зовсім не торкнулася. Чимало нинішніх керівників  підприємств – індустріальних монстрів часів СРСР – у своїй поведінці нагадують героя серіалу «Чорнобиль» Анатолія Дятлова, інженера ЧАЕС. Зухвалий, хамовитий, не схильний чути будь-яку думку, окрім власної – такий тип керівників виховували на так званих «закритих» підприємствах: оборонних заводах, атомних станціях тощо. Режим секретності й ласка від влади дозволяли поводитися з підлеглими саме так, як Дятлов. У сучасній Україні чимало таких директорів залишилося у південно-східних регіонах. Усі ці «міцні господарники», які на догоду своїм політичним партнерам з Партії регіонів завжди могли організувати кілька автобусів з робітниками – для Антимайдану чи якоїсь подібної акції. Великі металургійні, хімічні, машинобудівні підприємства залишаються потенційними «чорнобилями». Не в останню чергу – через керівників старої формації. Про складну екологічну ситуацію в Маріуполі писали дуже багато разів. Але, як не дивно, серед місцевих, особливо працівників ахметовських меткомбінтаів, найпоширенішим є настрій: «Так, викиди вбивають довкілля, але ж ці заводи дають нам роботу – то чи є сенс щось змінювати»? Люди озвучують позицію керівництва, не сміють йти проти неї. Так само, як молоді співробітники ЧАЕС у серіалі.  Навіть у комерційних фірмах таких некомпетентних самодурів немало. Як свідчать соцопитування конфлікти з керівництвом входить в ТОП-3 причин звільнення звільнення з роботи. Керівники навіть несвідомо копіюють цю радянську модель. По київських редакціях ходить легенда про редактора, який так само кричав і принижував підлеглих, як і Дятлов.   Хамство в розмовах звичне і для першого кабінету країни. Записи директора Кучми всі чули; Ющенко, попри інтелігентний вигляд, дозволяв собі на людях звертатися до незнайомих чиновників на «ти», кричати на них; про  Януковича кажуть, що міг і в морду дати (але доказів немає); за свідченнями оточення Порошенка, він теж досить зверхньо і зневажливо міг говорити зі своїми підлеглими та політичними партнерами. Реального Зеленського ми побачили під час першої спроби Радіо «Свобода» взяти в нього інтерв’ю. Потім цей нахабний і зневажливий стиль спілкування перейшов у відеозвернення.  Що там президенти! Подивіться, як говорять вчителі з дітьми у школах чи батьки на дитячих майданчиках. Таких «дятлових» сотні тисяч, і вони передають свою культуру (в розумінні набору навичок спілкування, звичок, світогляду тощо) наступним поколінням. Мабуть скрізь у світі є люди, які нехтують посадовими інструкціями з безпека. Але в Союзі це увійло в культ. Сцена в останній серії, коли працівник апаратної кричить, що цього робити не можна, але начальник каже \"Робіть!\" західним глядачем сприймається, мабуть, як виняткове самодурство. Та ми втаємничені і знаємо, що нехтування безпекою загальне правило на наших теренах. Крайнім і абсурдним проявом цього культурного феномену є відмова частини водіїв пристібатися пасками безпеки.  Чорнобиль породив не лише недовіру до чиновників, а й до техніки. Вагома частина серіалу присвячена пошуку причин аварії на ЧАЕС, і одна з них – дефект радянських атомних реакторів. Визнати це перед усім світом, з точки зору радянського керівництва, було неприпустимо – і навіть всередині країни говорити про це є зухвалим злочином. В українцях відтоді живе переконаність, що будь-яка техніка вітчизняного виробництва – неякісна і небезпечна для життя. З останніх прикладів – міномети «Молот»: чимало фахівців говорить про те, що фатальні вибухи під час стрільб найчастіше трапляються через помилки розрахунків (подвійне заряджання, наприклад). Проте, дуже популярним є настрій «це українські міномети такі неякісні».  Те саме стосується й цивільних сфер життя: австралійці можуть радісно зустрічати українську «Мрію». Українці ж воліють здебільшого не помічати жодних технологічних розробок на Батьківщині: дається взнаки скепсис, народжений в роки перебудов. Частково він породжений об’єктивно вищою якістю багатьох західних товарів, частково – розчаруваннями в радянському «науково-технічному прогресі», який обернувся страхітливою техногенною аварією. у фільмі присутня суб’єктність колонії Україна Фільмові інколи закидають: Україна в ньому, мовляв, як справжня колонія, позбавлена суб’єктності. Докір не зовсім справедливий. По-перше, станом на 1986-ий рік УРСР дійсно була позбавлена суб’єктності. По-друге, у фільмі присутня суб’єктність колонії. Один із сильних моментів фільму: діалог солдата й бабці, що не збирається залишати свою оселю та згадує усі трагедії, які пережила Україна в ХХ столітті. У цій бабусі легко вгадати нинішніх мешканців прифронтових сіл на Донбасі, які, хай там що, не збираються покидати свої помешкання. Укоріненість – це те, що не змінюється з роками. Війна – єдиний порівнюваний з Чорнобилем шок, який Україна пережила у новітній історії. І тоді, і тепер все значною мірою трималося на людях із розвиненим почуттям обов’язку: вчених, військових, добровольцях. Вони можуть входити у конфлікт із владою, не знати реального масштабу загроз, але виконувати свою справу під десантним гаслом «ніхто крім нас». З одним відчутним «але»: примусу в Україні значно менше, і ризикують життям ті, хто морально готовий для цього.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqgvtb-2Km-b",
        "colab_type": "text"
      },
      "source": [
        "# Clean data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNrY9Dg6Km-c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from spacy.lang.uk.stop_words import STOP_WORDS"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C_7BtlGGZ2Af",
        "colab": {}
      },
      "source": [
        "def clean_text(text, remove_stopwords = True):\n",
        "    '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''\n",
        "    \n",
        "    # Convert words to lower case\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Format words and remove unwanted characters\n",
        "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'\\<a href', ' ', text)\n",
        "    text = re.sub(r'&amp;', '', text) \n",
        "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
        "    text = re.sub(r'<br />', ' ', text)\n",
        "    text = re.sub(r'\\'', ' ', text)\n",
        "    pattern = re.compile('[\\W_]+')\n",
        "    text = pattern.sub(' ', text)\n",
        "    \n",
        "    # Optionally, remove stop words\n",
        "    if remove_stopwords:\n",
        "        text = text.split()\n",
        "        stops = STOP_WORDS\n",
        "        text = [w for w in text if not w in stops]\n",
        "        text = \" \".join(text)\n",
        "\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xq_sPmmQKm-h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "e5a2aea9-8409-422e-c7e8-8ca2efa6a161"
      },
      "source": [
        "clean_text(example_article)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'влада погано комунікує суспільством чуємо явища причин одна криється чорнобильській трагедії аварію причини наслідки попервах погано розуміли кремлі усіляко приховували применшували власне срср доброю традицією чаес першою техногенною катастрофою варто задати киштимську аварію уралі 1957 го масштаб інформаційний ефект чорнобиля нечуваний відтоді серед українців залишилася помітна травма трапляється надзвичайне влада вочевидь брехатиме викривлятиме інформацію соціумі велике поле чуток маніпуляцій добу соцмереж являються фейкові повідомлення стилі чоловік подруги працює пожежником чорнобильські зоні йдуть різні варіації жахалок заходить 30 вибуху аес надто цинічною брехня брехатимуть впевнені мільйони громадян незалежній україні тотальної цензури всемогутнього кгб імперських амбіцій чиновникам надзвичайних ситуацій доводиться докладати чимало зусиль викликати бодай якусь довіру порівняно недавня історія 2007 го села ожидів львівщині перекинулися цистерни фосфором генерал олександр кузьмук віце прем єром приїхав місце подій заспокоїти місцевих мешканців скуштував огірок городу місцевої мешканки погляд типова ляшковщина пережиток чорнобиля однойменному серіалі промовиста сцена шахтар кидає стіл московськими чиновниками респіратор словами якби респіратори допомагали носили знімаючи чорнобильська брехня породила недовіру представники влади змушені переконувати людей власним емпіричним прикладом бодай персональною присутністю торік масове отруєння дітей черкасах володимир гройсман особисто літав провідати люди молодшого покоління висміяли численних фотожабах покоління яке пережило чорнобиль особиста присутність очільника уряду стала переконливим спростуванням неймовірних чуток екологічну ситуацію місті серіалі доволі промовисто показано ієрархію радянського чиновництва дрібні місцеві функціонери прагнуть обдурити столичних шишок уникнути відповідальності зіграти некомпетентності високих гостей прип ятські можновладці зустрічають бориса щербину впевнені зуміють навіщати локшини вуха кремлівському партійному бонзі їхньому здивуванню меж товариш цк демонструє знання атомної енергетики українськими чиновниками візитерам києва прагнуть продемонструвати показуху пускати фасаду заколисувати їхню увагу замість ділової розмови перевести візит банкет п янку радянські прихвати помітити ледь кожній поїздці президента прем єра регіонами демократизація розпочалася часів горбачовської перебудови змусила політиків публічними червоних директорів вимога торкнулася чимало нинішніх керівників підприємств індустріальних монстрів часів срср поведінці нагадують героя серіалу чорнобиль анатолія дятлова інженера чаес зухвалий хамовитий схильний чути яку думку власної тип керівників виховували званих закритих підприємствах оборонних заводах атомних станціях тощо режим секретності влади дозволяли поводитися підлеглими дятлов сучасній україні чимало таких директорів залишилося південно східних регіонах міцні господарники догоду своїм політичним партнерам партії регіонів могли організувати автобусів робітниками антимайдану якоїсь подібної акції великі металургійні хімічні машинобудівні підприємства залишаються потенційними чорнобилями останню чергу керівників старої формації складну екологічну ситуацію маріуполі писали разів дивно серед місцевих працівників ахметовських меткомбінтаів найпоширенішим настрій викиди вбивають довкілля заводи дають роботу сенс змінювати люди озвучують позицію керівництва сміють йти молоді співробітники чаес серіалі комерційних фірмах таких некомпетентних самодурів немало свідчать соцопитування конфлікти керівництвом входить топ 3 причин звільнення звільнення роботи керівники несвідомо копіюють радянську модель київських редакціях ходить легенда редактора кричав принижував підлеглих дятлов хамство розмовах звичне першого кабінету країни записи директора кучми чули ющенко попри інтелігентний вигляд дозволяв людях звертатися незнайомих чиновників кричати януковича кажуть морду дати доказів свідченнями оточення порошенка зверхньо зневажливо говорити своїми підлеглими політичними партнерами реального зеленського побачили першої спроби радіо свобода взяти інтерв ю нахабний зневажливий стиль спілкування перейшов відеозвернення президенти подивіться говорять вчителі дітьми школах батьки дитячих майданчиках таких дятлових сотні передають культуру розумінні набору навичок спілкування звичок світогляду тощо наступним поколінням мабуть світі люди нехтують посадовими інструкціями безпека союзі увійло культ сцена останній серії працівник апаратної кричить робити начальник робіть західним глядачем сприймається мабуть виняткове самодурство втаємничені знаємо нехтування безпекою загальне правило теренах крайнім абсурдним проявом культурного феномену відмова частини водіїв пристібатися пасками безпеки чорнобиль породив недовіру чиновників техніки вагома частина серіалу присвячена пошуку причин аварії чаес одна дефект радянських атомних реакторів визнати світом точки зору радянського керівництва неприпустимо всередині країни говорити зухвалим злочином українцях відтоді живе переконаність техніка вітчизняного виробництва неякісна небезпечна життя останніх прикладів міномети молот чимало фахівців фатальні вибухи стрільб найчастіше трапляються помилки розрахунків подвійне заряджання наприклад популярним настрій українські міномети неякісні стосується цивільних сфер життя австралійці радісно зустрічати українську мрію українці воліють здебільшого помічати жодних технологічних розробок батьківщині дається взнаки скепсис народжений перебудов частково породжений об єктивно вищою якістю багатьох західних товарів частково розчаруваннями радянському науково технічному прогресі обернувся страхітливою техногенною аварією фільмі присутня суб єктність колонії україна фільмові інколи закидають україна мовляв справжня колонія позбавлена суб єктності докір справедливий перше станом 1986 ий урср позбавлена суб єктності друге фільмі присутня суб єктність колонії сильних моментів фільму діалог солдата бабці збирається залишати оселю згадує трагедії пережила україна хх столітті бабусі легко вгадати нинішніх мешканців прифронтових сіл донбасі хай збираються покидати помешкання укоріненість змінюється роками війна єдиний порівнюваний чорнобилем шок україна пережила новітній історії значною мірою трималося людях розвиненим почуттям обов язку вчених військових добровольцях входити конфлікт владою знати реального масштабу загроз виконувати справу десантним гаслом ніхто одним відчутним примусу україні значно ризикують життям морально готовий'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MZg_8goOZ4yy",
        "outputId": "d976c212-2a03-4718-a2d0-42e99ab29fc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Clean the summaries and texts\n",
        "clean_summaries = []\n",
        "for summary in articles.title:\n",
        "    clean_summaries.append(clean_text(summary, remove_stopwords=False))\n",
        "print(\"Summaries are complete.\")\n",
        "\n",
        "clean_texts = []\n",
        "for text in articles.article:\n",
        "    clean_texts.append(clean_text(text))\n",
        "print(\"Texts are complete.\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Summaries are complete.\n",
            "Texts are complete.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t1MO5QwNZ62b",
        "outputId": "529285d8-01a7-43cb-ac61-eed061ba431e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "# Example of cleaned article\n",
        "print(clean_summaries[0])\n",
        "print()\n",
        "print(clean_texts[0])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60 хвилин на реакцію як ефективно протидіяти дезінформації не запроваджуючи цензуру досвід тайваню\n",
            "\n",
            "одрі танг любить висловлюватися точно інтерв ю тайванський міністр портфеля табличці іменем танга написано цифровий міністр негайно виправляє згадуємо термін фейкові новини прийнятний термін дезінформація танг юридичне поняття тайвані тобто навмисна спрямована шкоду неправда найважливіше шкодить громадськості демократичній системі образу міністра сміхом шкодити міністру звичайна якісна журналістика відміну владних режимів азії таких сингапур тайвань вирішив боротися пошестю дезінформації вдаючись цензури арештів тайвань вийшов воєнного стану 1987 провів перші президентські вибори 1996 му попри економіка тайваню суттєво залежить торгівлі інвестицій китаю вважає тайвань своєю територією демократія продовжувала відокремлювати політичної системи континенті виборці тайваню обирають партією гоміндан схильна тісніших стосунків китаєм демократичною прогресивною партією дпп наголошує автономії прямій незалежності китаю чинна президентка цай інвень належить дпп танг програмістка вийшла хакерського середовища минулого тижня зустрілася журналістом комітету захисту журналістів тайпеї мета поспілкуватися тайвань намагається зберегти чесність демократичність змі противагу своєму значно більшому опоненту китаю жорстко контролює власні медіа потенційну змогу посіяти хаос відкритій медійній системі тайваню вважаю тайвані найбільшою мірою покладаємося власну здатність суспільства виявляти дезінформацію тобто навмисну спрямовану завдання шкоди неправду противагу журналістській роботі легко тридцять тридцять п ять тайвань перебуває китайська народна республіка людей старше покоління мають труднощі розрізненням дезінформації справжньої журналістської роботи державні змі єдиними змі відверто кажучи чимало пропаганди тож відмінності побачити людей народилися здобували освіту скасування воєнного стану тобто 80 х широкий діапазон інформаційних джерел їхній вибір демократія розпочалася перших президентських виборів 1996 му збіглася часі появою world wide web тож люди пов язують демократію демократизацією джерел інформації дезінформація небезпека відкритих суспільств тайваню режимів кнр використовують дезінформацію привід держави запровадити цензуру хочемо йти шляхом пам ятаємо воєнний стан перше масового поширення пропагандистської дезінформаційної кампанії спостерігаємо появу відправної точки здійснюють певне тестування перевірку різних варіантів стануть справді популярними тестуються меми різні версії побачити отримають вірусне поширення міністерств команду відповідальну разі виявлення дезінформаційної кампанії сягнула мас протягом 60 хвилин виготовити переконливе повідомлення короткий фільм картка медіа допис соціальних мережах міністр робить стрім президент прийшла комедійне шоу прем єр міністр дивиться стрім відеогри виявили робимо більшість населення отримує повідомлення щеплення дійде дезінформація тож захист працює подібно вакцини ведемо зразок таблиці обліку визначаємо міністерству реагування міністерству внутрішніх середньому 60 хвилин міністерству здоров добробуту середньому 70 змагаються своєрідній дружній конкуренції реагують швидше швидше вийдуть 60 хвилинну межу передають меседжі платформу миттєвих повідомлень line facebook екаунти президента цай інвень прем єра віце прем єра кожний велику кількість підписників суті просять фоловерів поширити роз яснення надійшло їхніх друзів родичів повідомлення дезінформацією традиційні змі звісно отримують контрповідомлення завдяки готують збалансовані публікації побачили випадку запустили контрповідомлення приготували відеоролики фільми картинки годин починається новий цикл новин змі протидія безнадійна чесно кажучи насправді виснажує деякі стають вірусними непомітно відбувається каналах застосовують наскрізне шифрування відміну фейсбук твіттер систем працює індексація гугл канали недоступні пошукових механізмів закрита кімната легко провести мутацію дезінформації потужний мем перш випустять мовити світ широкий розробили співпраці соціальною мережею line facebook систему яку називаємо сповіщення громадське сповіщення система працює подібно спам фільтра отримали електронний лист думаєте спам сміттєва реклама теоретично приватні комунікації держава переглядати пошту думаєте лист надійшов якоїсь країни принцеса хоче поділитися п ятьма мільйонами доларів можете позначити лист спам двохтисячних інтернет спільнота переконали операторів електронної пошти додати таку кнопку інтерфейсу ставили помітку спам пересилали текст повідомлення обов язково добровільне пожертвування глобальній системі назвою spamhaus список блокування доменів тощо існує ціла система імунна система електронної пошти значна кількість людей ставить прапорець спам робить кореляцію відправником листа відправляє наступного листа доходить адресата цензури нема потрапляє папки сміттєвою поштою замовчанням відбирати людей розробляємо схожу систему люди пересилати онлайнову інформацію повідомлення отримані системах негайних повідомлень спеціальному боту наразі популярний бот називається cofact тобто collaborative fact колективний факт скоро червні соціальна система line вбудує функціональність зробити натиснути повідомлення можете позначити дезінформацію наприклад справді популярна чутка випадку землетрусу потужнішого балів сусідні держави надсилати команди рятувальників згоди країни постраждала землетрусу привід окупації розумієте бог знає поширювали повідомлення якому випадку популярне повідомлення тож тайванський центр фактчекінгу досліджує міжнародні угоди договори підписані міністром закордонних подібні речі цитати джерела цитат зрештою заявляє повідомлення фальшивкою зробили facebook пообіцяв червня уточнять алгоритм facebook припинив поширювати повідомлення популярне стрічці цензурою подивитеся стрічку друга повідомлення залишилося містить попередження перевірка фактів встановила неправдивість напрацьовуємо схожі домовленості іншми соціальними мережами line популярна азії система обміну миттєвими повідомленнями прим facebook долучилися впровадження системи сповіщення громадське сповіщення ніяк системою сповіщення вилучення інформації примітка редактора речник line повідомив телефоном cpj співпрацюють групами факт чекінгу такими cofact ｍygopen taiwan factcheck center rumor truth задля створення офіційного екаунту line перевірки фактів користувачі зможуть пересилати сумнівну інформацію робити запит перевірку відповідності фактам facebook відповів негайно запит електронною поштою коментар виборів діє спеціальний набір правил обмежує пожертви кампанію відповідно відзначили іноземні кошти надходять шляхом виявлено надають перевагу закупівлі точно націленої реклами соціальних мережах традиційних змі тож кажемо пожертви кампанію слід розкрити інформацію дані пожертви місцеве населення право витрачати гроші фінансувати політичну рекламу рекламне агентство посередник повинен повідомити звідки надійшли кошти боротьба відмиванням грошей кінці ланцюжка виявиться іноземець кнр макао гонконг насправді злочином вибори особливою ситуацією захищаємо виборчий процес значно вищому рівіні звичайна система повідомлень громадських повідомлень\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhLMPnSjKm-u",
        "colab_type": "text"
      },
      "source": [
        "## Count the number of occurrences of each word in a set of text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aKm8RMdya1Rh",
        "colab": {}
      },
      "source": [
        "def count_words(count_dict, text):\n",
        "    '''Count the number of occurrences of each word in a set of text'''\n",
        "    for sentence in text:\n",
        "        for word in sentence.split():\n",
        "            if word not in count_dict:\n",
        "                count_dict[word] = 1\n",
        "            else:\n",
        "                count_dict[word] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JG4zOLkKm-x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "232f8aba-c772-4d9e-f4b9-3bc6554317ab"
      },
      "source": [
        "mydict = {}\n",
        "count_words(mydict, [\"ми будемо їсти піццу сьогодні ввечері\", \"їсти овочі корисно\"])\n",
        "mydict"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'будемо': 1,\n",
              " 'ввечері': 1,\n",
              " 'корисно': 1,\n",
              " 'ми': 1,\n",
              " 'овочі': 1,\n",
              " 'піццу': 1,\n",
              " 'сьогодні': 1,\n",
              " 'їсти': 2}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RYMNdxR-a3R4",
        "outputId": "a1458a85-d884-47bb-da2c-d2368aa870e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "word_counts = {}\n",
        "\n",
        "count_words(word_counts, clean_summaries)\n",
        "count_words(word_counts, clean_texts)\n",
        "            \n",
        "print(\"Size of Vocabulary:\", len(word_counts))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of Vocabulary: 158085\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF2iALt_Km-4",
        "colab_type": "text"
      },
      "source": [
        "# Load embeddings for Ukrainian language"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmJC2fa7RlKO",
        "colab_type": "text"
      },
      "source": [
        "To run this, you need to download the embedding into the folder 'deep_learning'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Es_I5U0xa5S5",
        "outputId": "9be60540-0d40-4343-eecf-d7cc6511441f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "embeddings_index = {}\n",
        "with open('news.lowercased.tokenized.word2vec.300d') as f:\n",
        "    for line in f:\n",
        "        values = line.split(' ')\n",
        "        word = values[0]\n",
        "        embedding = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = embedding\n",
        "\n",
        "print('Word embeddings:', len(embeddings_index))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word embeddings: 328959\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWfbarm7Km-_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0e8b426b-3893-4144-9b6d-619630f3c2fb"
      },
      "source": [
        "# explore e,bedding dimention\n",
        "embeddings_index[\"мама\"].shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hh9fAIniKm_D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AW802uiNa75r",
        "outputId": "a9148318-0027-4f90-a0b9-b161c06fefdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "missing_words = 0\n",
        "threshold = 20\n",
        "\n",
        "for word, count in word_counts.items():\n",
        "    if count > threshold:\n",
        "        if word not in embeddings_index:\n",
        "            missing_words += 1\n",
        "            \n",
        "missing_ratio = round(missing_words/len(word_counts),4)*100\n",
        "            \n",
        "print(\"Number of words missing from Word2Vec:\", missing_words)\n",
        "print(\"Percent of words that are missing from vocabulary: {}%\".format(missing_ratio))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words missing from Word2Vec: 61\n",
            "Percent of words that are missing from vocabulary: 0.04%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IulvgG3PKm_K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1075
        },
        "outputId": "1ab33cf2-147c-4730-bea4-43911cc0fb36"
      },
      "source": [
        "missing_words = []\n",
        "for word, count in word_counts.items():\n",
        "    if count > threshold and word not in embeddings_index:\n",
        "        missing_words.append((word,count))\n",
        "missing_words"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('гриняка', 22),\n",
              " ('австро', 33),\n",
              " ('none', 161),\n",
              " ('width', 232),\n",
              " ('parent', 53),\n",
              " ('container', 159),\n",
              " ('height', 67),\n",
              " ('5em', 53),\n",
              " ('bottom', 106),\n",
              " ('7em', 53),\n",
              " ('font', 318),\n",
              " ('weight', 106),\n",
              " ('align', 106),\n",
              " ('18px', 106),\n",
              " ('1em', 53),\n",
              " ('verdana', 53),\n",
              " ('arial', 54),\n",
              " ('serif', 53),\n",
              " ('383e47', 53),\n",
              " ('decoration', 53),\n",
              " ('background', 53),\n",
              " ('ffc423', 53),\n",
              " ('padding', 53),\n",
              " ('10px', 58),\n",
              " ('outline', 53),\n",
              " ('cursor', 53),\n",
              " ('pointer', 53),\n",
              " ('2em', 53),\n",
              " ('навʼязливої', 53),\n",
              " ('жовто', 21),\n",
              " ('києво', 31),\n",
              " ('клікайте', 49),\n",
              " ('title', 22),\n",
              " ('аннабелла', 21),\n",
              " ('вынтэнэ', 46),\n",
              " ('уладзімер', 32),\n",
              " ('костанчук', 26),\n",
              " ('сингапуре', 24),\n",
              " ('арина', 27),\n",
              " ('оборота', 23),\n",
              " ('нерозважальна', 64),\n",
              " ('eugenelakinsky', 80),\n",
              " ('пейпел', 85),\n",
              " ('емейлу', 79),\n",
              " ('u336801545841', 26),\n",
              " ('вебмані', 26),\n",
              " ('u0456', 53),\n",
              " ('u043b', 33),\n",
              " ('u043a', 30),\n",
              " ('u0441', 31),\n",
              " ('u0442', 39),\n",
              " ('u0437', 21),\n",
              " ('u0430', 54),\n",
              " ('u0440', 37),\n",
              " ('u0435', 25),\n",
              " ('u043e', 63),\n",
              " ('u0432', 40),\n",
              " ('u043d', 38),\n",
              " ('u0438', 40),\n",
              " ('u0434', 26),\n",
              " ('одрина', 25)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nm5EVcm-Km_N",
        "colab_type": "text"
      },
      "source": [
        "# Words to indexes, indexes to words dicts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Kmqng2XHh6_p",
        "outputId": "bc923ecf-1669-494d-a449-9d86f03a1e1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "vocab_to_int = {} \n",
        "\n",
        "value = 0\n",
        "for word, count in word_counts.items():\n",
        "    if count >= threshold or word in embeddings_index:\n",
        "        vocab_to_int[word] = value\n",
        "        value += 1\n",
        "\n",
        "# Special tokens that will be added to our vocab\n",
        "codes = [\"<UNK>\",\"<PAD>\",\"<EOS>\",\"<GO>\"]   \n",
        "\n",
        "# Add codes to vocab\n",
        "for code in codes:\n",
        "    vocab_to_int[code] = len(vocab_to_int)\n",
        "\n",
        "# Dictionary to convert integers to words\n",
        "int_to_vocab = {}\n",
        "for word, value in vocab_to_int.items():\n",
        "    int_to_vocab[value] = word\n",
        "\n",
        "usage_ratio = round(len(vocab_to_int) / len(word_counts),4)*100\n",
        "\n",
        "print(\"Total number of unique words:\", len(word_counts))\n",
        "print(\"Number of words we will use:\", len(vocab_to_int))\n",
        "print(\"Percent of words we will use: {}%\".format(usage_ratio))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of unique words: 158085\n",
            "Number of words we will use: 116515\n",
            "Percent of words we will use: 73.7%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoX0pSC7Km_S",
        "colab_type": "text"
      },
      "source": [
        "# Create word embedding matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rwT6WIjDh-5I",
        "outputId": "3df954c9-3322-4abe-b3c3-49ce13b8ef7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "embedding_dim = 300\n",
        "nb_words = len(vocab_to_int)\n",
        "\n",
        "# Create matrix with default values of zero\n",
        "word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\n",
        "for word, i in vocab_to_int.items():\n",
        "    if word in embeddings_index:\n",
        "        word_embedding_matrix[i] = embeddings_index[word]\n",
        "    else:\n",
        "        # If word not in CN, create a random embedding for it\n",
        "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
        "        embeddings_index[word] = new_embedding\n",
        "        word_embedding_matrix[i] = new_embedding\n",
        "\n",
        "# Check if value matches len(vocab_to_int)\n",
        "print(len(word_embedding_matrix))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "116515\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypp0K6Z8Km_W",
        "colab_type": "text"
      },
      "source": [
        "# Convert sentences to sequence of words indexes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2gobgfFZiCHR",
        "colab": {}
      },
      "source": [
        "def convert_to_ints(text, word_count, unk_count, eos=False):\n",
        "    '''Convert words in text to an integer.\n",
        "       If word is not in vocab_to_int, use UNK's integer.\n",
        "       Total the number of words and UNKs.\n",
        "       Add EOS token to the end of texts'''\n",
        "    ints = []\n",
        "    for sentence in text:\n",
        "        sentence_ints = []\n",
        "        for word in sentence.split():\n",
        "            word_count += 1\n",
        "            if word in vocab_to_int:\n",
        "                sentence_ints.append(vocab_to_int[word])\n",
        "            else:\n",
        "                sentence_ints.append(vocab_to_int[\"<UNK>\"])\n",
        "                unk_count += 1\n",
        "        if eos:\n",
        "            sentence_ints.append(vocab_to_int[\"<EOS>\"])\n",
        "        ints.append(sentence_ints)\n",
        "    return ints, word_count, unk_count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uHOLyENmiFmo",
        "outputId": "83099482-89b0-4d5a-9ea9-38972794155f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "word_count = 0\n",
        "unk_count = 0\n",
        "\n",
        "int_summaries, word_count, unk_count = convert_to_ints(clean_summaries, word_count, unk_count)\n",
        "int_texts, word_count, unk_count = convert_to_ints(clean_texts, word_count, unk_count, eos=True)\n",
        "\n",
        "unk_percent = round(unk_count/word_count,4)*100\n",
        "\n",
        "print(\"Total number of words in headlines:\", word_count)\n",
        "print(\"Total number of UNKs in headlines:\", unk_count)\n",
        "print(\"Percent of words that are UNK: {}%\".format(unk_percent))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of words in headlines: 1156199\n",
            "Total number of UNKs in headlines: 52944\n",
            "Percent of words that are UNK: 4.58%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QCptg3QKm_h",
        "colab_type": "text"
      },
      "source": [
        "# Get the length of each sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2OD95WMaiIVK",
        "colab": {}
      },
      "source": [
        "def create_lengths(text):\n",
        "    '''Create a data frame of the sentence lengths from a text'''\n",
        "    lengths = []\n",
        "    for sentence in text:\n",
        "        lengths.append(len(sentence))\n",
        "    return pd.DataFrame(lengths, columns=['counts'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W21ABIG1Km_m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "3743ed20-d287-444f-890b-c334081f42f2"
      },
      "source": [
        "create_lengths(int_summaries[:3])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>counts</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   counts\n",
              "0      13\n",
              "1      13\n",
              "2      11"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IBDnDWjfiMXu",
        "outputId": "0420af12-4ee3-4567-a277-36926e8cf4ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "source": [
        "lengths_summaries = create_lengths(int_summaries)\n",
        "lengths_texts = create_lengths(int_texts)\n",
        "\n",
        "print(\"Summaries:\")\n",
        "print(lengths_summaries.describe())\n",
        "print()\n",
        "print(\"Texts:\")\n",
        "print(lengths_texts.describe())"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Summaries:\n",
            "            counts\n",
            "count  1783.000000\n",
            "mean     11.479529\n",
            "std       3.173526\n",
            "min       2.000000\n",
            "25%       9.000000\n",
            "50%      11.000000\n",
            "75%      13.000000\n",
            "max      25.000000\n",
            "\n",
            "Texts:\n",
            "            counts\n",
            "count  1783.000000\n",
            "mean    637.977566\n",
            "std     546.913226\n",
            "min       1.000000\n",
            "25%     203.000000\n",
            "50%     536.000000\n",
            "75%     917.500000\n",
            "max    4219.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEWMXsVjKm_s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "1c79b797-5e35-4e1c-9249-8cca85a89ac9"
      },
      "source": [
        "# Inspect the length of texts\n",
        "print(np.percentile(lengths_texts.counts, 89.5))\n",
        "print(np.percentile(lengths_texts.counts, 95))\n",
        "print(np.percentile(lengths_texts.counts, 99))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1310.3400000000006\n",
            "1638.8999999999999\n",
            "2431.400000000005\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZlSENdvKm_u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "567077e7-f9ea-4bf0-e925-643f2c4ead6c"
      },
      "source": [
        "# Inspect the length of summaries\n",
        "print(np.percentile(lengths_summaries.counts, 90))\n",
        "print(np.percentile(lengths_summaries.counts, 95))\n",
        "print(np.percentile(lengths_summaries.counts, 99))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16.0\n",
            "17.0\n",
            "19.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1nXakxkKm_y",
        "colab_type": "text"
      },
      "source": [
        "# Count the number of time UNKNOWN appears in a sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9ftG9iqtiRhC",
        "colab": {}
      },
      "source": [
        "def unk_counter(sentence):\n",
        "    '''Counts the number of time UNK appears in a sentence.'''\n",
        "    unk_count = 0\n",
        "    for word in sentence:\n",
        "        if word == vocab_to_int[\"<UNK>\"]:\n",
        "            unk_count += 1\n",
        "    return unk_count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tklM0U49Km_0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ad3cdedb-07a1-4da9-ab04-1e07f78c32d1"
      },
      "source": [
        "max_text_length = 2000 # This will cover up to 89.5% lengthes\n",
        "max_summary_length = 20 # This will cover up to 99% lengthes\n",
        "min_length = 2\n",
        "unk_text_limit = 20 # text can contain up to 1 UNK word\n",
        "unk_summary_limit = 5 # Summary should not contain any UNK word\n",
        "\n",
        "def filter_condition(item):\n",
        "    int_summary = item[0]\n",
        "    int_text = item[1]\n",
        "    if(len(int_summary) >= min_length and \n",
        "       len(int_summary) <= max_summary_length and \n",
        "       len(int_text) >= min_length and \n",
        "       len(int_text) <= max_text_length and \n",
        "       unk_counter(int_summary) <= unk_summary_limit and \n",
        "       unk_counter(int_text) <= unk_text_limit):\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "int_text_summaries = list(zip(int_summaries , int_texts))\n",
        "int_text_summaries_filtered = list(filter(filter_condition, int_text_summaries))\n",
        "sorted_int_text_summaries = sorted(int_text_summaries_filtered, key=lambda item: len(item[1]))\n",
        "sorted_int_text_summaries = list(zip(*sorted_int_text_summaries))\n",
        "sorted_summaries = list(sorted_int_text_summaries[0])\n",
        "sorted_texts = list(sorted_int_text_summaries[1])\n",
        "# Delete those temporary varaibles\n",
        "del int_text_summaries, sorted_int_text_summaries, int_text_summaries_filtered\n",
        "# Compare lengths to ensure they match\n",
        "print(len(sorted_summaries))\n",
        "print(len(sorted_texts))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1027\n",
            "1027\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15snkGgYKm_4",
        "colab_type": "text"
      },
      "source": [
        "# Building the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-JU3DbUGibgt",
        "colab": {}
      },
      "source": [
        "def model_inputs():\n",
        "    '''Create palceholders for inputs to the model'''\n",
        "    \n",
        "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
        "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
        "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
        "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
        "    summary_length = tf.placeholder(tf.int32, (None,), name='summary_length')\n",
        "    max_summary_length = tf.reduce_max(summary_length, name='max_dec_len')\n",
        "    text_length = tf.placeholder(tf.int32, (None,), name='text_length')\n",
        "\n",
        "    return input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E75uw3LQirgX",
        "colab": {}
      },
      "source": [
        "def process_encoding_input(target_data, vocab_to_int, batch_size):\n",
        "    '''Remove the last word id from each batch and concat the <GO> to the begining of each batch'''\n",
        "    \n",
        "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
        "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
        "\n",
        "    return dec_input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RRmHqVSCiujL",
        "colab": {}
      },
      "source": [
        "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob):\n",
        "    '''Create the encoding layer'''\n",
        "    \n",
        "    for layer in range(num_layers):\n",
        "        with tf.variable_scope('encoder_{}'.format(layer)):\n",
        "            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
        "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
        "            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \n",
        "                                                    input_keep_prob = keep_prob)\n",
        "\n",
        "            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
        "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
        "            cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, \n",
        "                                                    input_keep_prob = keep_prob)\n",
        "\n",
        "            enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n",
        "                                                                    cell_bw, \n",
        "                                                                    rnn_inputs,\n",
        "                                                                    sequence_length,\n",
        "                                                                    dtype=tf.float32)\n",
        "    # Join outputs since we are using a bidirectional RNN\n",
        "    enc_output = tf.concat(enc_output,2)\n",
        "    \n",
        "    return enc_output, enc_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o312UorKiyhE",
        "colab": {}
      },
      "source": [
        "def training_decoding_layer(dec_embed_input, summary_length, dec_cell, initial_state, output_layer, \n",
        "                            vocab_size, max_summary_length):\n",
        "    '''Create the training logits'''\n",
        "    \n",
        "    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
        "                                                        sequence_length=summary_length,\n",
        "                                                        time_major=False)\n",
        "\n",
        "    training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
        "                                                       training_helper,\n",
        "                                                       initial_state,\n",
        "                                                       output_layer) \n",
        "\n",
        "    training_logits, _ , _ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
        "                                                           output_time_major=False,\n",
        "                                                           impute_finished=True,\n",
        "                                                           maximum_iterations=max_summary_length)\n",
        "    return training_decoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lJfDRX6yi1dD",
        "colab": {}
      },
      "source": [
        "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, initial_state, output_layer,\n",
        "                             max_summary_length, batch_size):\n",
        "    '''Create the inference logits'''\n",
        "    \n",
        "    start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
        "    \n",
        "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
        "                                                                start_tokens,\n",
        "                                                                end_token)\n",
        "                \n",
        "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
        "                                                        inference_helper,\n",
        "                                                        initial_state,\n",
        "                                                        output_layer)\n",
        "                \n",
        "    inference_logits, _ , _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
        "                                                            output_time_major=False,\n",
        "                                                            impute_finished=True,\n",
        "                                                            maximum_iterations=max_summary_length)\n",
        "    \n",
        "    return inference_decoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bPQ4wxdyi37i",
        "colab": {}
      },
      "source": [
        "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, text_length, summary_length, \n",
        "                   max_summary_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers):\n",
        "    '''Create the decoding cell and attention for the training and inference decoding layers'''\n",
        "    \n",
        "    for layer in range(num_layers):\n",
        "        with tf.variable_scope('decoder_{}'.format(layer)):\n",
        "            lstm = tf.contrib.rnn.LSTMCell(rnn_size,\n",
        "                                           initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
        "            dec_cell = tf.contrib.rnn.DropoutWrapper(lstm, \n",
        "                                                     input_keep_prob = keep_prob)\n",
        "    \n",
        "    output_layer = Dense(vocab_size,\n",
        "                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
        "    \n",
        "    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n",
        "                                                  enc_output,\n",
        "                                                  text_length,\n",
        "                                                  normalize=False,\n",
        "                                                  name='BahdanauAttention')\n",
        "\n",
        "    dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell,\n",
        "                                                          attn_mech,\n",
        "                                                          rnn_size)\n",
        "            \n",
        "    #initial_state = tf.contrib.seq2seq.AttentionWrapperState(enc_state[0],\n",
        "    #                                                                _zero_state_tensors(rnn_size, \n",
        "    #                                                                                    batch_size, \n",
        "    #                                                                                    tf.float32)) \n",
        "    initial_state = dec_cell.zero_state(batch_size=batch_size,dtype=tf.float32).clone(cell_state=enc_state[0])\n",
        "\n",
        "    with tf.variable_scope(\"decode\"):\n",
        "        training_decoder = training_decoding_layer(dec_embed_input, \n",
        "                                                  summary_length, \n",
        "                                                  dec_cell, \n",
        "                                                  initial_state,\n",
        "                                                  output_layer,\n",
        "                                                  vocab_size, \n",
        "                                                  max_summary_length)\n",
        "        \n",
        "        training_logits,_ ,_ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
        "                                  output_time_major=False,\n",
        "                                  impute_finished=True,\n",
        "                                  maximum_iterations=max_summary_length)\n",
        "    with tf.variable_scope(\"decode\", reuse=True):\n",
        "        inference_decoder = inference_decoding_layer(embeddings,  \n",
        "                                                    vocab_to_int['<GO>'], \n",
        "                                                    vocab_to_int['<EOS>'],\n",
        "                                                    dec_cell, \n",
        "                                                    initial_state, \n",
        "                                                    output_layer,\n",
        "                                                    max_summary_length,\n",
        "                                                    batch_size)\n",
        "        \n",
        "        inference_logits,_ ,_ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
        "                                  output_time_major=False,\n",
        "                                  impute_finished=True,\n",
        "                                  maximum_iterations=max_summary_length)\n",
        "\n",
        "    return training_logits, inference_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "orftVP-AjqNF",
        "colab": {}
      },
      "source": [
        "def seq2seq_model(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length, \n",
        "                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size):\n",
        "    '''Use the previous functions to create the training and inference logits'''\n",
        "    \n",
        "    # Use Numberbatch's embeddings and the newly created ones as our embeddings\n",
        "    embeddings = word_embedding_matrix\n",
        "    \n",
        "    enc_embed_input = tf.nn.embedding_lookup(embeddings, input_data)\n",
        "    enc_output, enc_state = encoding_layer(rnn_size, text_length, num_layers, enc_embed_input, keep_prob)\n",
        "    \n",
        "    dec_input = process_encoding_input(target_data, vocab_to_int, batch_size)\n",
        "    dec_embed_input = tf.nn.embedding_lookup(embeddings, dec_input)\n",
        "    \n",
        "    training_logits, inference_logits  = decoding_layer(dec_embed_input, \n",
        "                                                        embeddings,\n",
        "                                                        enc_output,\n",
        "                                                        enc_state, \n",
        "                                                        vocab_size, \n",
        "                                                        text_length, \n",
        "                                                        summary_length, \n",
        "                                                        max_summary_length,\n",
        "                                                        rnn_size, \n",
        "                                                        vocab_to_int, \n",
        "                                                        keep_prob, \n",
        "                                                        batch_size,\n",
        "                                                        num_layers)\n",
        "    \n",
        "    return training_logits, inference_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xXDlzjChjwan",
        "colab": {}
      },
      "source": [
        "def pad_sentence_batch(sentence_batch):\n",
        "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
        "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
        "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6HHWN1-Ojyzd",
        "colab": {}
      },
      "source": [
        "def get_batches(summaries, texts, batch_size):\n",
        "    \"\"\"Batch summaries, texts, and the lengths of their sentences together\"\"\"\n",
        "    for batch_i in range(0, len(texts)//batch_size):\n",
        "        start_i = batch_i * batch_size\n",
        "        summaries_batch = summaries[start_i:start_i + batch_size]\n",
        "        texts_batch = texts[start_i:start_i + batch_size]\n",
        "        pad_summaries_batch = np.array(pad_sentence_batch(summaries_batch))\n",
        "        pad_texts_batch = np.array(pad_sentence_batch(texts_batch))\n",
        "        \n",
        "        # Need the lengths for the _lengths parameters\n",
        "        pad_summaries_lengths = []\n",
        "        for summary in pad_summaries_batch:\n",
        "            pad_summaries_lengths.append(len(summary))\n",
        "        \n",
        "        pad_texts_lengths = []\n",
        "        for text in pad_texts_batch:\n",
        "            pad_texts_lengths.append(len(text))\n",
        "        \n",
        "        yield pad_summaries_batch, pad_texts_batch, pad_summaries_lengths, pad_texts_lengths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NGYG2IOfj10V",
        "colab": {}
      },
      "source": [
        "epochs = 50\n",
        "batch_size = 64\n",
        "rnn_size = 256\n",
        "num_layers = 2\n",
        "learning_rate = 0.005\n",
        "keep_probability = 0.75"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gs_vtjzzKnAY",
        "colab_type": "text"
      },
      "source": [
        "# Build graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uxuqzSkzj6yG",
        "outputId": "dfb59f02-dfbd-44aa-bfa2-599f6e33f9f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        }
      },
      "source": [
        "# Build the graph\n",
        "train_graph = tf.Graph()\n",
        "# Set the graph to default to ensure that it is ready for training\n",
        "with train_graph.as_default():\n",
        "    \n",
        "    # Load the model inputs    \n",
        "    input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length = model_inputs()\n",
        "\n",
        "    # Create the training and inference logits\n",
        "    training_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
        "                                                      targets, \n",
        "                                                      keep_prob,   \n",
        "                                                      text_length,\n",
        "                                                      summary_length,\n",
        "                                                      max_summary_length,\n",
        "                                                      len(vocab_to_int)+1,\n",
        "                                                      rnn_size, \n",
        "                                                      num_layers, \n",
        "                                                      vocab_to_int,\n",
        "                                                      batch_size)\n",
        "    \n",
        "    # Create tensors for the training logits and inference logits\n",
        "    training_logits = tf.identity(training_logits.rnn_output, 'logits')\n",
        "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
        "    \n",
        "    # Create the weights for sequence_loss\n",
        "    masks = tf.sequence_mask(summary_length, max_summary_length, dtype=tf.float32, name='masks')\n",
        "\n",
        "    with tf.name_scope(\"optimization\"):\n",
        "        # Loss function\n",
        "        cost = tf.contrib.seq2seq.sequence_loss(\n",
        "            training_logits,\n",
        "            targets,\n",
        "            masks)\n",
        "\n",
        "        # Optimizer\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "\n",
        "        # Gradient Clipping\n",
        "        gradients = optimizer.compute_gradients(cost)\n",
        "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
        "        train_op = optimizer.apply_gradients(capped_gradients)\n",
        "print(\"Graph is built.\")"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/embedding_ops.py:132: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-34-9390cb26606a>:7: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-34-9390cb26606a>:20: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Graph is built.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sS17Zn9dKnAc",
        "colab_type": "text"
      },
      "source": [
        "# Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iSkOPDO3kADU",
        "outputId": "2402c2b4-54fa-4696-a365-0be92ee6528b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#start = 200000\n",
        "#end = start + 300000\n",
        "#sorted_summaries_short = sorted_summaries[start:end]\n",
        "#sorted_texts_short = sorted_texts[start:end]\n",
        "sorted_summaries_short = sorted_summaries\n",
        "sorted_texts_short = sorted_texts\n",
        "print(\"The shortest text length:\", len(sorted_texts_short[0]))\n",
        "print(\"The longest text length:\",len(sorted_texts_short[-1]))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The shortest text length: 2\n",
            "The longest text length: 1737\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkpvOd_6KnAh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e62bc27a-45d4-4059-bb05-ca9ad5044874"
      },
      "source": [
        "per_epoch = 6\n",
        "update_check = (len(sorted_texts_short)//batch_size//per_epoch)-1\n",
        "update_check"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zw6Wa7HjKnAk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "outputId": "fbc1e8fc-e15b-41bb-f905-fcb9e1d94023"
      },
      "source": [
        "# Train the Model\n",
        "learning_rate_decay = 0.95\n",
        "min_learning_rate = 0.0005\n",
        "display_step = 20 # Check training loss after every 20 batches\n",
        "stop_early = 0 \n",
        "stop = 3 # If the update loss does not decrease in 3 consecutive update checks, stop training\n",
        "per_epoch = 6 # Make 3 update checks per epoch\n",
        "update_check = (len(sorted_texts_short)//batch_size//per_epoch)-1\n",
        "\n",
        "update_loss = 0 \n",
        "batch_loss = 0\n",
        "summary_update_loss = [] # Record the update losses for saving improvements in the model\n",
        "\n",
        "checkpoint = \"best_model.ckpt\" \n",
        "with tf.Session(graph=train_graph) as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    # If we want to continue training a previous session\n",
        "    #loader = tf.train.import_meta_graph(\"./\" + checkpoint + '.meta')\n",
        "    #loader.restore(sess, checkpoint)\n",
        "    \n",
        "    for epoch_i in range(1, epochs+1):\n",
        "        update_loss = 0\n",
        "        batch_loss = 0\n",
        "        for batch_i, (summaries_batch, texts_batch, summaries_lengths, texts_lengths) in enumerate(\n",
        "                get_batches(sorted_summaries_short, sorted_texts_short, batch_size)):\n",
        "            start_time = time.time()\n",
        "            _, loss = sess.run(\n",
        "                [train_op, cost],\n",
        "                {input_data: texts_batch,\n",
        "                 targets: summaries_batch,\n",
        "                 lr: learning_rate,\n",
        "                 summary_length: summaries_lengths,\n",
        "                 text_length: texts_lengths,\n",
        "                 keep_prob: keep_probability})\n",
        "\n",
        "            batch_loss += loss\n",
        "            update_loss += loss\n",
        "            end_time = time.time()\n",
        "            batch_time = end_time - start_time\n",
        "\n",
        "            if batch_i % display_step == 0 and batch_i > 0:\n",
        "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
        "                      .format(epoch_i,\n",
        "                              epochs, \n",
        "                              batch_i, \n",
        "                              len(sorted_texts_short) // batch_size, \n",
        "                              batch_loss / display_step, \n",
        "                              batch_time*display_step))\n",
        "                batch_loss = 0\n",
        "\n",
        "            if batch_i % update_check == 0 and batch_i > 0:\n",
        "                print(\"Average loss for this update:\", round(update_loss/update_check,3))\n",
        "                summary_update_loss.append(update_loss)\n",
        "                \n",
        "                # If the update loss is at a new minimum, save the model\n",
        "                if update_loss <= min(summary_update_loss):\n",
        "                    print('New Record!') \n",
        "                    stop_early = 0\n",
        "                    saver = tf.train.Saver() \n",
        "                    saver.save(sess, checkpoint)\n",
        "\n",
        "                else:\n",
        "                    print(\"No Improvement.\")\n",
        "                    stop_early += 1\n",
        "                    if stop_early == stop:\n",
        "                        break\n",
        "                update_loss = 0\n",
        "            \n",
        "                    \n",
        "        # Reduce learning rate, but not below its minimum value\n",
        "        learning_rate *= learning_rate_decay\n",
        "        if learning_rate < min_learning_rate:\n",
        "            learning_rate = min_learning_rate\n",
        "        \n",
        "        if stop_early == stop:\n",
        "            print(\"Stopping Training.\")\n",
        "            break"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average loss for this update: 21.827\n",
            "New Record!\n",
            "Average loss for this update: 8.968\n",
            "New Record!\n",
            "Average loss for this update: 7.351\n",
            "New Record!\n",
            "Average loss for this update: 7.227\n",
            "New Record!\n",
            "Average loss for this update: 7.317\n",
            "No Improvement.\n",
            "Average loss for this update: 8.0\n",
            "No Improvement.\n",
            "Average loss for this update: 8.003\n",
            "No Improvement.\n",
            "Stopping Training.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6lwZqK_hkKss",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17457
        },
        "outputId": "33626f07-a410-41cc-a6c2-e32c025e0adf"
      },
      "source": [
        "checkpoint = \"best_model.ckpt\" \n",
        "\n",
        "loaded_graph = tf.Graph()\n",
        "with tf.Session(graph=loaded_graph) as sess:\n",
        "    # Load saved model\n",
        "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
        "    loader.restore(sess, checkpoint)\n",
        "    names = []\n",
        "    [names.append(n.name) for n in loaded_graph.as_graph_def().node]\n",
        "names"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from best_model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['input',\n",
              " 'targets',\n",
              " 'learning_rate',\n",
              " 'keep_prob',\n",
              " 'summary_length',\n",
              " 'Const',\n",
              " 'max_dec_len',\n",
              " 'text_length',\n",
              " 'ReverseV2/axis',\n",
              " 'ReverseV2',\n",
              " 'embedding_lookup/params_0',\n",
              " 'embedding_lookup/axis',\n",
              " 'embedding_lookup',\n",
              " 'embedding_lookup/Identity',\n",
              " 'encoder_0/DropoutWrapperInit/Const',\n",
              " 'encoder_0/DropoutWrapperInit/Const_1',\n",
              " 'encoder_0/DropoutWrapperInit_1/Const',\n",
              " 'encoder_0/DropoutWrapperInit_1/Const_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Rank',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/range/start',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/range/delta',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/range',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/concat/values_0',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/concat/axis',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/concat',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/transpose',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/sequence_length',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Shape',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice/stack',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice/stack_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice/stack_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims/dim',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/Const',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/concat/axis',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/concat',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/zeros/Const',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/zeros',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_1/dim',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/Const_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_2/dim',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/Const_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/concat_1/axis',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/concat_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/zeros_1/Const',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/zeros_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_3/dim',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/Const_3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Shape_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/stack',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Equal',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Const',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/All',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Assert/Const',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Assert/Const_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Assert/Assert/data_0',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Assert/Assert/data_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Assert/Assert',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/CheckSeqLen',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Shape_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice_1/stack',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice_1/stack_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice_1/stack_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Shape_3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice_2/stack',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice_2/stack_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice_2/stack_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/ExpandDims/dim',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/ExpandDims',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Const_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/concat_1/axis',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/concat_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/zeros/Const',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/zeros',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Const_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Min',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Const_3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Max',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/time',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArray',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArray_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayUnstack/Shape',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayUnstack/strided_slice/stack',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayUnstack/strided_slice/stack_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayUnstack/strided_slice/stack_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayUnstack/strided_slice',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayUnstack/range/start',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayUnstack/range/delta',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayUnstack/range',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Maximum/x',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Maximum',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Minimum',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/iteration_counter',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Enter',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Enter_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Enter_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Enter_3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Enter_4',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Merge',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Merge_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Merge_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Merge_3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Merge_4',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Less/Enter',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Less',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Less_1/Enter',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Less_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/LogicalAnd',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/LoopCond',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Switch',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Switch_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Switch_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Switch_3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Switch_4',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Identity',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Identity_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Identity_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Identity_3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Identity_4',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/add/y',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/add',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/TensorArrayReadV3/Enter',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/TensorArrayReadV3/Enter_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/TensorArrayReadV3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/GreaterEqual/Enter',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/GreaterEqual',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/sub/x',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/sub/Enter',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/sub',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/Shape',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/sub/x',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/sub',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/random_uniform/min',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/random_uniform/max',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/random_uniform/RandomUniform',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/random_uniform/sub',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/random_uniform/mul',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/random_uniform',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/add',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/Floor',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/truediv',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/mul',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/shape',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/min',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/max',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/RandomUniform',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/sub',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/mul',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/kernel',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/kernel/IsInitialized/VarIsInitializedOp',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/kernel/Assign',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/kernel/Read/ReadVariableOp',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/kernel/Read/Identity',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/bias/Initializer/zeros/shape_as_tensor',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/bias/Initializer/zeros/Const',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/bias/Initializer/zeros',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/bias',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/bias/IsInitialized/VarIsInitializedOp',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/bias/Assign',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/bias/Read/ReadVariableOp',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/bias/Read/Identity',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/concat/axis',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/concat',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/MatMul/Enter',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/MatMul',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/BiasAdd/Enter',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/BiasAdd',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/Const',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/split/split_dim',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/split',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/add/y',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/add',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/Sigmoid',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/mul',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/Sigmoid_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/Tanh',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/mul_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/add_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/Sigmoid_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/Tanh_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/mul_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Select/Enter',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Select',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Select_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Select_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/TensorArrayWrite/TensorArrayWriteV3/Enter',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/TensorArrayWrite/TensorArrayWriteV3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/add_1/y',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/add_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/NextIteration',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/NextIteration_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/NextIteration_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/NextIteration_3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/NextIteration_4',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Exit',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Exit_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Exit_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Exit_3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Exit_4',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayStack/TensorArraySizeV3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayStack/range/start',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayStack/range/delta',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayStack/range',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayStack/TensorArrayGatherV3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Const_4',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Rank_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/range_1/start',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/range_1/delta',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/range_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/concat_2/values_0',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/concat_2/axis',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/concat_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/transpose_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/ReverseSequence',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Rank',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/range/start',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/range/delta',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/range',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/concat/values_0',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/concat/axis',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/concat',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/transpose',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/sequence_length',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Shape',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice/stack',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice/stack_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice/stack_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims/dim',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/Const',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/concat/axis',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/concat',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/zeros/Const',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/zeros',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_1/dim',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/Const_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_2/dim',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/Const_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/concat_1/axis',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/concat_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/zeros_1/Const',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/zeros_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_3/dim',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/Const_3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Shape_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/stack',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Equal',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Const',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/All',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Assert/Const',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Assert/Const_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Assert/Assert/data_0',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Assert/Assert/data_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Assert/Assert',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/CheckSeqLen',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Shape_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice_1/stack',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice_1/stack_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice_1/stack_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Shape_3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice_2/stack',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice_2/stack_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice_2/stack_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/ExpandDims/dim',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/ExpandDims',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Const_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/concat_1/axis',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/concat_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/zeros/Const',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/zeros',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Const_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Min',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Const_3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Max',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/time',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArray',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArray_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayUnstack/Shape',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayUnstack/strided_slice/stack',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayUnstack/strided_slice/stack_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayUnstack/strided_slice/stack_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayUnstack/strided_slice',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayUnstack/range/start',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayUnstack/range/delta',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayUnstack/range',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Maximum/x',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Maximum',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Minimum',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/iteration_counter',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Enter',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Enter_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Enter_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Enter_3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Enter_4',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Merge',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Merge_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Merge_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Merge_3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Merge_4',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Less/Enter',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Less',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Less_1/Enter',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Less_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/LogicalAnd',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/LoopCond',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Switch',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Switch_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Switch_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Switch_3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Switch_4',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Identity',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Identity_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Identity_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Identity_3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Identity_4',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/add/y',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/add',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/TensorArrayReadV3/Enter',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/TensorArrayReadV3/Enter_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/TensorArrayReadV3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/GreaterEqual/Enter',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/GreaterEqual',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/sub/x',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/sub/Enter',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/sub',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/Shape',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/sub/x',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/sub',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/random_uniform/min',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/random_uniform/max',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/random_uniform/RandomUniform',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/random_uniform/sub',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/random_uniform/mul',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/random_uniform',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/add',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/Floor',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/truediv',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/mul',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/shape',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/min',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/max',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/RandomUniform',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/sub',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/mul',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/kernel',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/kernel/IsInitialized/VarIsInitializedOp',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/kernel/Assign',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/kernel/Read/ReadVariableOp',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/kernel/Read/Identity',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/bias/Initializer/zeros/shape_as_tensor',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/bias/Initializer/zeros/Const',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/bias/Initializer/zeros',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/bias',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/bias/IsInitialized/VarIsInitializedOp',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/bias/Assign',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/bias/Read/ReadVariableOp',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/bias/Read/Identity',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/concat/axis',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/concat',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/MatMul/Enter',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/MatMul',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/BiasAdd/Enter',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/BiasAdd',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/Const',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/split/split_dim',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/split',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/add/y',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/add',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/Sigmoid',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/mul',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/Sigmoid_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/Tanh',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/mul_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/add_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/Sigmoid_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/Tanh_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/mul_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Select/Enter',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Select',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Select_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Select_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/TensorArrayWrite/TensorArrayWriteV3/Enter',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/TensorArrayWrite/TensorArrayWriteV3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/add_1/y',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/add_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/NextIteration',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/NextIteration_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/NextIteration_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/NextIteration_3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/NextIteration_4',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Exit',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Exit_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Exit_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Exit_3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Exit_4',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayStack/TensorArraySizeV3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayStack/range/start',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayStack/range/delta',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayStack/range',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayStack/TensorArrayGatherV3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Const_4',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Rank_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/range_1/start',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/range_1/delta',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/range_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/concat_2/values_0',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/concat_2/axis',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/concat_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/transpose_1',\n",
              " 'encoder_0/ReverseSequence',\n",
              " 'encoder_1/DropoutWrapperInit/Const',\n",
              " 'encoder_1/DropoutWrapperInit/Const_1',\n",
              " 'encoder_1/DropoutWrapperInit_1/Const',\n",
              " 'encoder_1/DropoutWrapperInit_1/Const_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Rank',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/range/start',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/range/delta',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/range',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/concat/values_0',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/concat/axis',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/concat',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/transpose',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/sequence_length',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Shape',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice/stack',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice/stack_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice/stack_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims/dim',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/Const',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/concat/axis',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/concat',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/zeros/Const',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/zeros',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_1/dim',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/Const_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_2/dim',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/Const_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/concat_1/axis',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/concat_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/zeros_1/Const',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/zeros_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_3/dim',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/Const_3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Shape_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/stack',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Equal',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Const',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/All',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Assert/Const',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Assert/Const_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Assert/Assert/data_0',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Assert/Assert/data_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Assert/Assert',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/CheckSeqLen',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Shape_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice_1/stack',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice_1/stack_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice_1/stack_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Shape_3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice_2/stack',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice_2/stack_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice_2/stack_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/ExpandDims/dim',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/ExpandDims',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Const_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/concat_1/axis',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/concat_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/zeros/Const',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/zeros',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Const_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Min',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Const_3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Max',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/time',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArray',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArray_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayUnstack/Shape',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayUnstack/strided_slice/stack',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayUnstack/strided_slice/stack_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayUnstack/strided_slice/stack_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayUnstack/strided_slice',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayUnstack/range/start',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayUnstack/range/delta',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayUnstack/range',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Maximum/x',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Maximum',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Minimum',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/iteration_counter',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Enter',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Enter_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Enter_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Enter_3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Enter_4',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Merge',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Merge_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Merge_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Merge_3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Merge_4',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Less/Enter',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Less',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Less_1/Enter',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Less_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/LogicalAnd',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/LoopCond',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Switch',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Switch_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Switch_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Switch_3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Switch_4',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Identity',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Identity_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Identity_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Identity_3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Identity_4',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/add/y',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/add',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/TensorArrayReadV3/Enter',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/TensorArrayReadV3/Enter_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/TensorArrayReadV3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/GreaterEqual/Enter',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/GreaterEqual',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/sub/x',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/sub/Enter',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/sub',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/Shape',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/sub/x',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/sub',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/random_uniform/min',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/random_uniform/max',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/random_uniform/RandomUniform',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/random_uniform/sub',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/random_uniform/mul',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/random_uniform',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/add',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/Floor',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/truediv',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/mul',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/shape',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/min',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/max',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/RandomUniform',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/sub',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/mul',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/kernel',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/kernel/IsInitialized/VarIsInitializedOp',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/kernel/Assign',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/kernel/Read/ReadVariableOp',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/kernel/Read/Identity',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/bias/Initializer/zeros/shape_as_tensor',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/bias/Initializer/zeros/Const',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/bias/Initializer/zeros',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/bias',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/bias/IsInitialized/VarIsInitializedOp',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/bias/Assign',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/bias/Read/ReadVariableOp',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/bias/Read/Identity',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/concat/axis',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/concat',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/MatMul/Enter',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/MatMul',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/BiasAdd/Enter',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/BiasAdd',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/Const',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/split/split_dim',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/split',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/add/y',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/add',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/Sigmoid',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/mul',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/Sigmoid_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/Tanh',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/mul_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/add_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/Sigmoid_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/Tanh_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/mul_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Select/Enter',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Select',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Select_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Select_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/TensorArrayWrite/TensorArrayWriteV3/Enter',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/TensorArrayWrite/TensorArrayWriteV3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/add_1/y',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/add_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/NextIteration',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/NextIteration_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/NextIteration_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/NextIteration_3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/NextIteration_4',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_4',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayStack/TensorArraySizeV3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayStack/range/start',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayStack/range/delta',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayStack/range',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayStack/TensorArrayGatherV3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Const_4',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Rank_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/range_1/start',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/range_1/delta',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/range_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/concat_2/values_0',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/concat_2/axis',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/concat_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/transpose_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/ReverseSequence',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Rank',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/range/start',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/range/delta',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/range',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/concat/values_0',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/concat/axis',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/concat',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/transpose',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/sequence_length',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Shape',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice/stack',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice/stack_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice/stack_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims/dim',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/Const',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/concat/axis',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/concat',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/zeros/Const',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/zeros',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_1/dim',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/Const_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_2/dim',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/Const_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/concat_1/axis',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/concat_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/zeros_1/Const',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/zeros_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_3/dim',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/Const_3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Shape_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/stack',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Equal',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Const',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/All',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Assert/Const',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Assert/Const_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Assert/Assert/data_0',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Assert/Assert/data_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Assert/Assert',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/CheckSeqLen',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Shape_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice_1/stack',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice_1/stack_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice_1/stack_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Shape_3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice_2/stack',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice_2/stack_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice_2/stack_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/ExpandDims/dim',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/ExpandDims',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Const_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/concat_1/axis',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/concat_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/zeros/Const',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/zeros',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Const_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Min',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Const_3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Max',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/time',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArray',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArray_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayUnstack/Shape',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayUnstack/strided_slice/stack',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayUnstack/strided_slice/stack_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayUnstack/strided_slice/stack_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayUnstack/strided_slice',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayUnstack/range/start',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayUnstack/range/delta',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayUnstack/range',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Maximum/x',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Maximum',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Minimum',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/iteration_counter',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Enter',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Enter_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Enter_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Enter_3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Enter_4',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Merge',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Merge_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Merge_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Merge_3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Merge_4',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Less/Enter',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Less',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Less_1/Enter',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Less_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/LogicalAnd',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/LoopCond',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Switch',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Switch_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Switch_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Switch_3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Switch_4',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Identity',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Identity_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Identity_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Identity_3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Identity_4',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/add/y',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/add',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/TensorArrayReadV3/Enter',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/TensorArrayReadV3/Enter_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/TensorArrayReadV3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/GreaterEqual/Enter',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/GreaterEqual',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/sub/x',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/sub/Enter',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/sub',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/Shape',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/sub/x',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/sub',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/random_uniform/min',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/random_uniform/max',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/random_uniform/RandomUniform',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/random_uniform/sub',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/random_uniform/mul',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/random_uniform',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/add',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/Floor',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/truediv',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/mul',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/shape',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/min',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/max',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/RandomUniform',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/sub',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/mul',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/kernel',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/kernel/IsInitialized/VarIsInitializedOp',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/kernel/Assign',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/kernel/Read/ReadVariableOp',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/kernel/Read/Identity',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/bias/Initializer/zeros/shape_as_tensor',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/bias/Initializer/zeros/Const',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/bias/Initializer/zeros',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/bias',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/bias/IsInitialized/VarIsInitializedOp',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/bias/Assign',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/bias/Read/ReadVariableOp',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/bias/Read/Identity',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/concat/axis',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/concat',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/MatMul/Enter',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/MatMul',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/BiasAdd/Enter',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/BiasAdd',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/Const',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/split/split_dim',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/split',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/add/y',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/add',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/Sigmoid',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/mul',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/Sigmoid_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/Tanh',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/mul_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/add_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/Sigmoid_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/Tanh_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/mul_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Select/Enter',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Select',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Select_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Select_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/TensorArrayWrite/TensorArrayWriteV3/Enter',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/TensorArrayWrite/TensorArrayWriteV3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/add_1/y',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/add_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/NextIteration',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/NextIteration_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/NextIteration_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/NextIteration_3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/NextIteration_4',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Exit',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Exit_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Exit_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Exit_3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Exit_4',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayStack/TensorArraySizeV3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayStack/range/start',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayStack/range/delta',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayStack/range',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayStack/TensorArrayGatherV3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Const_4',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Rank_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/range_1/start',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/range_1/delta',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/range_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/concat_2/values_0',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/concat_2/axis',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/concat_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/transpose_1',\n",
              " 'encoder_1/ReverseSequence',\n",
              " 'concat/axis',\n",
              " 'concat',\n",
              " 'StridedSlice/begin',\n",
              " 'StridedSlice/end',\n",
              " 'StridedSlice/strides',\n",
              " 'StridedSlice',\n",
              " 'Fill/dims',\n",
              " 'Fill/value',\n",
              " 'Fill',\n",
              " 'concat_1/axis',\n",
              " 'concat_1',\n",
              " 'embedding_lookup_1/params_0',\n",
              " 'embedding_lookup_1/axis',\n",
              " 'embedding_lookup_1',\n",
              " 'embedding_lookup_1/Identity',\n",
              " 'decoder_0/DropoutWrapperInit/Const',\n",
              " 'decoder_0/DropoutWrapperInit/Const_1',\n",
              " 'decoder_1/DropoutWrapperInit/Const',\n",
              " 'decoder_1/DropoutWrapperInit/Const_1',\n",
              " 'BahdanauAttention/Shape',\n",
              " 'BahdanauAttention/strided_slice/stack',\n",
              " 'BahdanauAttention/strided_slice/stack_1',\n",
              " 'BahdanauAttention/strided_slice/stack_2',\n",
              " 'BahdanauAttention/strided_slice',\n",
              " 'BahdanauAttention/SequenceMask/Const',\n",
              " 'BahdanauAttention/SequenceMask/Const_1',\n",
              " 'BahdanauAttention/SequenceMask/Range',\n",
              " 'BahdanauAttention/SequenceMask/ExpandDims/dim',\n",
              " 'BahdanauAttention/SequenceMask/ExpandDims',\n",
              " 'BahdanauAttention/SequenceMask/Cast',\n",
              " 'BahdanauAttention/SequenceMask/Less',\n",
              " 'BahdanauAttention/SequenceMask/Cast_1',\n",
              " 'BahdanauAttention/Shape_1',\n",
              " 'BahdanauAttention/strided_slice_1/stack',\n",
              " 'BahdanauAttention/strided_slice_1/stack_1',\n",
              " 'BahdanauAttention/strided_slice_1/stack_2',\n",
              " 'BahdanauAttention/strided_slice_1',\n",
              " 'BahdanauAttention/ones/shape_as_tensor',\n",
              " 'BahdanauAttention/ones/Const',\n",
              " 'BahdanauAttention/ones',\n",
              " 'BahdanauAttention/Shape_2',\n",
              " 'BahdanauAttention/strided_slice_2/stack',\n",
              " 'BahdanauAttention/strided_slice_2/stack_1',\n",
              " 'BahdanauAttention/strided_slice_2/stack_2',\n",
              " 'BahdanauAttention/strided_slice_2',\n",
              " 'BahdanauAttention/assert_equal/Equal',\n",
              " 'BahdanauAttention/assert_equal/Const',\n",
              " 'BahdanauAttention/assert_equal/All',\n",
              " 'BahdanauAttention/assert_equal/Assert/Const',\n",
              " 'BahdanauAttention/assert_equal/Assert/Const_1',\n",
              " 'BahdanauAttention/assert_equal/Assert/Const_2',\n",
              " 'BahdanauAttention/assert_equal/Assert/Const_3',\n",
              " 'BahdanauAttention/assert_equal/Assert/Assert/data_0',\n",
              " 'BahdanauAttention/assert_equal/Assert/Assert/data_1',\n",
              " 'BahdanauAttention/assert_equal/Assert/Assert/data_2',\n",
              " 'BahdanauAttention/assert_equal/Assert/Assert/data_4',\n",
              " 'BahdanauAttention/assert_equal/Assert/Assert',\n",
              " 'BahdanauAttention/Shape_3',\n",
              " 'BahdanauAttention/concat/axis',\n",
              " 'BahdanauAttention/concat',\n",
              " 'BahdanauAttention/Reshape',\n",
              " 'BahdanauAttention/mul',\n",
              " 'memory_layer/kernel/Initializer/random_uniform/shape',\n",
              " 'memory_layer/kernel/Initializer/random_uniform/min',\n",
              " 'memory_layer/kernel/Initializer/random_uniform/max',\n",
              " 'memory_layer/kernel/Initializer/random_uniform/RandomUniform',\n",
              " 'memory_layer/kernel/Initializer/random_uniform/sub',\n",
              " 'memory_layer/kernel/Initializer/random_uniform/mul',\n",
              " 'memory_layer/kernel/Initializer/random_uniform',\n",
              " 'memory_layer/kernel',\n",
              " 'memory_layer/kernel/Assign',\n",
              " 'memory_layer/kernel/read',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/axes',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/free',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/Shape',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/GatherV2/axis',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/GatherV2',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/GatherV2_1/axis',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/GatherV2_1',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/Const',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/Prod',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/Const_1',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/Prod_1',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/concat/axis',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/concat',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/stack',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/transpose',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/Reshape',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/transpose_1/perm',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/transpose_1',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/Reshape_1/shape',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/Reshape_1',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/MatMul',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/Const_2',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/concat_1/axis',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/concat_1',\n",
              " 'BahdanauAttention/memory_layer/Tensordot',\n",
              " 'BahdanauAttention/Shape_4',\n",
              " 'BahdanauAttention/strided_slice_3/stack',\n",
              " 'BahdanauAttention/strided_slice_3/stack_1',\n",
              " 'BahdanauAttention/strided_slice_3/stack_2',\n",
              " 'BahdanauAttention/strided_slice_3',\n",
              " 'BahdanauAttention/Shape_5',\n",
              " 'BahdanauAttention/strided_slice_4/stack',\n",
              " 'BahdanauAttention/strided_slice_4/stack_1',\n",
              " 'BahdanauAttention/strided_slice_4/stack_2',\n",
              " 'BahdanauAttention/strided_slice_4',\n",
              " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/Const',\n",
              " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/Const_1',\n",
              " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/concat/axis',\n",
              " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/concat',\n",
              " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/zeros/Const',\n",
              " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/zeros',\n",
              " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/Const_2',\n",
              " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/Const_3',\n",
              " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/Const_4',\n",
              " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/Const_5',\n",
              " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/concat_1/axis',\n",
              " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/concat_1',\n",
              " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/zeros_1/Const',\n",
              " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/zeros_1',\n",
              " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/Const_6',\n",
              " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/Const_7',\n",
              " 'AttentionWrapperZeroState/assert_equal/x',\n",
              " 'AttentionWrapperZeroState/assert_equal/Equal',\n",
              " 'AttentionWrapperZeroState/assert_equal/Const',\n",
              " 'AttentionWrapperZeroState/assert_equal/All',\n",
              " 'AttentionWrapperZeroState/assert_equal/Assert/Const',\n",
              " 'AttentionWrapperZeroState/assert_equal/Assert/Const_1',\n",
              " 'AttentionWrapperZeroState/assert_equal/Assert/Const_2',\n",
              " 'AttentionWrapperZeroState/assert_equal/Assert/Const_3',\n",
              " 'AttentionWrapperZeroState/assert_equal/Assert/Assert/data_0',\n",
              " 'AttentionWrapperZeroState/assert_equal/Assert/Assert/data_1',\n",
              " 'AttentionWrapperZeroState/assert_equal/Assert/Assert/data_2',\n",
              " 'AttentionWrapperZeroState/assert_equal/Assert/Assert/data_4',\n",
              " 'AttentionWrapperZeroState/assert_equal/Assert/Assert',\n",
              " 'AttentionWrapperZeroState/checked_cell_state',\n",
              " 'AttentionWrapperZeroState/checked_cell_state_1',\n",
              " 'AttentionWrapperZeroState/Const',\n",
              " 'AttentionWrapperZeroState/ExpandDims/dim',\n",
              " 'AttentionWrapperZeroState/ExpandDims',\n",
              " 'AttentionWrapperZeroState/concat/axis',\n",
              " 'AttentionWrapperZeroState/concat',\n",
              " 'AttentionWrapperZeroState/zeros/Const',\n",
              " 'AttentionWrapperZeroState/zeros',\n",
              " 'AttentionWrapperZeroState/Const_1',\n",
              " 'AttentionWrapperZeroState/ExpandDims_1/dim',\n",
              " 'AttentionWrapperZeroState/ExpandDims_1',\n",
              " 'AttentionWrapperZeroState/zeros_1',\n",
              " 'AttentionWrapperZeroState/Const_2',\n",
              " 'AttentionWrapperZeroState/Const_3',\n",
              " 'AttentionWrapperZeroState/concat_1/axis',\n",
              " 'AttentionWrapperZeroState/concat_1',\n",
              " 'AttentionWrapperZeroState/zeros_2/Const',\n",
              " 'AttentionWrapperZeroState/zeros_2',\n",
              " 'AttentionWrapperZeroState/Const_4',\n",
              " 'AttentionWrapperZeroState/Const_5',\n",
              " 'AttentionWrapperZeroState/Const_6',\n",
              " 'AttentionWrapperZeroState/ExpandDims_2/dim',\n",
              " 'AttentionWrapperZeroState/ExpandDims_2',\n",
              " 'AttentionWrapperZeroState/concat_2/axis',\n",
              " 'AttentionWrapperZeroState/concat_2',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVjKXeoxKnAp",
        "colab_type": "text"
      },
      "source": [
        "# Generating Summaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d6abG-TxkW78",
        "colab": {}
      },
      "source": [
        "def text_to_seq(text):\n",
        "    '''Prepare the text for the model'''\n",
        "    \n",
        "    text = clean_text(text)\n",
        "    return [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in text.split()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yUEenOMrlLT2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "063d19ba-fe22-422b-8cdb-4a796233c584"
      },
      "source": [
        "# Create your own review or use one from the dataset\n",
        "#input_sentence = \"I have never eaten an apple before, but this red one was nice. \\\n",
        "                  #I think that I will try a green apple next time.\"\n",
        "#text = text_to_seq(input_sentence)\n",
        "random = np.random.randint(0,len(clean_texts))\n",
        "input_sentence = clean_texts[random]\n",
        "text = text_to_seq(clean_texts[random])\n",
        "\n",
        "checkpoint = \"best_model.ckpt\"\n",
        "\n",
        "loaded_graph = tf.Graph()\n",
        "with tf.Session(graph=loaded_graph) as sess:\n",
        "    # Load saved model\n",
        "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
        "    loader.restore(sess, checkpoint)\n",
        "\n",
        "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
        "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
        "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
        "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
        "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
        "    \n",
        "    #Multiply by batch_size to match the model's input parameters\n",
        "    answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
        "                                      summary_length: [np.random.randint(5,8)], \n",
        "                                      text_length: [len(text)]*batch_size,\n",
        "                                      keep_prob: 1.0})[0] \n",
        "\n",
        "# Remove the padding from the tweet\n",
        "pad = vocab_to_int[\"<PAD>\"] \n",
        "\n",
        "print('Original Text:', articles.article[random])\n",
        "print('Original summary:', articles.title[random])#clean_summaries[random]\n",
        "\n",
        "print('\\nText')\n",
        "print('  Word Ids:    {}'.format([i for i in text]))\n",
        "print('  Input Words: {}'.format(\" \".join([int_to_vocab[i] for i in text])))\n",
        "\n",
        "print('\\nSummary')\n",
        "print('  Word Ids:       {}'.format([i for i in answer_logits if i != pad]))\n",
        "print('  Response Words: {}'.format(\" \".join([int_to_vocab[i] for i in answer_logits if i != pad])))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from best_model.ckpt\n",
            "Original Text: Так, міська влада вже не виключає того, що в самому центрі міста можуть з’явитися щонайменше два підземні паркінги — під Михайлівською та Європейською площами. Про це «Українській правді. Київ» недавно розповів радник мера Києва Павло Рябікін. Очевидно, хочуть «прощупати», як на цю божевільну ідею відреагує громадськість. Рябікін бачить лише два шляхи вирішення проблем із паркінгами — впорядкувати наявний паркувальний простір і створити додаткові паркувальні потужності, у тому числі у центрі міста. Про інші способи зменшити кількість автівок радник Кличка чомусь не згадує. Насправді таких альтернатив уже набереться з добрий десяток: це і платний в’їзд у центр, і обмеження за парними/непарними номерними знаками, ускладнення в’їзду великих авто (наприклад, джипів)? пільги для суперміні- та електромобілів тощо. У різному вигляді такі практики чудово зарекомендували себе в багатьох європейських містах — Лондоні, Парижі, Амстердамі, Берліні, Пекіні та десятках інших міст. У нас же так: багато авто — значить треба більше парковок! І добре ще, якщо вони легальні. Законні паркінги часто стоять напівпорожні. Який дурень захоче платити, коли можна стати на тротуарі? Тому не дивно, що поки влада бездіє, цілі площі в центрі Києва перетворюються на стихійні парковки. На Михайлівській та Софійській площі щодня паркуються десятки автомобілів, які не тільки псують туристичну привабливість міста, але й навіть не платять жодної гривні в міський бюджет.  Щойно новорічні гуляння на Михайлівській та Софійській площах закінчилися, машини одразу ж повернулися на місце. Однак не за один день. «Теорія розбитих вікон» в дії: спочатку з’являється перший ряд автівок. Коли їхніх власників не штрафують, з’являються другий, третій ряди і т.д., поки заставленим не виявиться майже увесь простір. До розбитого шлагбауму вже понад місяць нікому немає діла Софійська площа, 4 лютого. Праворуч видно третій ряд, на якому поки що лише кілька автомобілів... Через тиждень, 12 лютого, третій ряд уже майже заповнений А нижче фото новорічного дива. Софіївський майдан із парковки перетворився на місце, де гуляють і відпочивають  люди. Новорічні свята — чи не єдиний час, коли площі використовують люди, а не автівки. Скріншоти з відео «Радіо Свободи» Подібна ситуація і на Михайлівській площі. Водії сприймають лінії, що їх утворює чорна плитка, як розмітку на паркінгу. Перетягуйте курсором лінію по середині ілюстрації  І якщо влітку 2014 року, через кілька місяців після обрання на посаду, Кличко піарився тим, що на Михайлівській нема машин, то зараз, через півтора року, такими «дрібничками» уже ніхто не заморочується. Адже вибори поки що дуже далеко.\n",
            "Original summary: Як Михайлівська і Софіївські площі перетворюються у стоянки. Києву варто забути про безкоштовні парковки у центрі\n",
            "\n",
            "Text\n",
            "  Word Ids:    [6964, 505, 22647, 2921, 1407, 30937, 3295, 44196, 62907, 79187, 70768, 28625, 592, 21069, 1294, 16492, 9828, 19394, 6408, 755, 18120, 79188, 13029, 1466, 116511, 79189, 6648, 14911, 932, 79188, 8860, 1445, 680, 882, 62922, 29587, 22935, 3174, 4867, 425, 10450, 62860, 19540, 9116, 2921, 1407, 4196, 2591, 1248, 47622, 19394, 5662, 15153, 5657, 1936, 8103, 42653, 4954, 67562, 11446, 13168, 57410, 10338, 2116, 8032, 79190, 79191, 79192, 47694, 46706, 11644, 2896, 2112, 8359, 79193, 5524, 116511, 31436, 8331, 14921, 9470, 12240, 7345, 59053, 1138, 1279, 2114, 6431, 7487, 62964, 7488, 79194, 52916, 706, 2112, 58583, 35721, 19081, 62907, 11996, 79195, 24004, 11093, 2923, 79, 37954, 8673, 505, 116511, 2581, 3281, 2921, 755, 4507, 41050, 2838, 65808, 63237, 3281, 2610, 37365, 5029, 2528, 44339, 60284, 41751, 1407, 644, 6197, 3593, 3618, 734, 14376, 4726, 61643, 65808, 63237, 53877, 30989, 5278, 9611, 11988, 1398, 26661, 49308, 11835, 1997, 877, 17762, 47622, 4595, 7011, 63445, 5028, 20574, 11543, 116511, 8449, 4867, 59499, 60566, 608, 1917, 2010, 2571, 63249, 9576, 1771, 3151, 16842, 12098, 17762, 7520, 2528, 4397, 1699, 3151, 17762, 4954, 30711, 212, 79196, 16467, 79197, 313, 2838, 617, 1398, 19697, 23930, 1419, 4726, 4727, 585, 3281, 1674, 1419, 22302, 1844, 1157, 4298, 4792, 23526, 9498, 65808, 3281, 24883, 10766, 306, 33601, 938, 13864, 55294, 62916, 116511, 116511, 4133, 13817, 13286, 13225, 1493, 1136, 9272, 4529, 64700, 79198, 65808, 3525, 6557, 1542, 2885, 79199, 4954, 2160, 116511, 315]\n",
            "  Input Words: міська влада виключає центрі міста явитися щонайменше підземні паркінги михайлівською європейською площами українській правді київ недавно розповів радник мера києва павло рябікін очевидно хочуть <UNK> божевільну ідею відреагує громадськість рябікін бачить шляхи вирішення проблем паркінгами впорядкувати наявний паркувальний простір створити додаткові паркувальні потужності числі центрі міста способи зменшити кількість автівок радник кличка чомусь згадує насправді таких альтернатив уже набереться добрий десяток платний їзд центр обмеження парними непарними номерними знаками ускладнення їзду великих авто наприклад джипів пільги <UNK> електромобілів тощо різному вигляді практики чудово зарекомендували багатьох європейських містах лондоні парижі амстердамі берліні пекіні десятках міст авто парковок легальні законні паркінги стоять напівпорожні дурень захоче платити стати тротуарі дивно влада <UNK> цілі площі центрі києва перетворюються стихійні парковки михайлівській софійській площі щодня паркуються десятки автомобілів псують туристичну привабливість міста платять жодної гривні міський бюджет щойно новорічні гуляння михайлівській софійській площах закінчилися машини одразу повернулися місце теорія розбитих вікон дії являється ряд автівок їхніх власників штрафують являються ряди д <UNK> виявиться простір розбитого шлагбауму понад місяць нікому діла софійська площа 4 лютого праворуч видно ряд якому автомобілів тиждень 12 лютого ряд уже заповнений фото новорічного дива софіївський майдан парковки перетворився місце гуляють відпочивають люди новорічні свята єдиний площі використовують люди автівки скріншоти відео радіо свободи подібна ситуація михайлівській площі водії сприймають лінії утворює чорна плитка розмітку паркінгу <UNK> <UNK> лінію середині ілюстрації влітку 2014 місяців обрання посаду кличко піарився михайлівській нема машин півтора такими дрібничками уже ніхто <UNK> вибори\n",
            "\n",
            "Summary\n",
            "  Word Ids:       [39837, 90995, 4978, 16912, 8020, 17869]\n",
            "  Response Words: ворожості паркові чоловіками пустого опустилися підказати\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h5zINNiDlQXZ",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}