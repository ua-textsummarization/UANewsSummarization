{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization of Github issues\n",
    "In this notebook I will write summaries with the help of my Seq2Seq model in Summarizer.py.\n",
    "\n",
    "The model works impressively well in the end!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from collections import Counter\n",
    "from langdetect import detect\n",
    "\n",
    "import Summarizer\n",
    "import summarizer_data_utils\n",
    "import summarizer_model_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data\n",
    "\n",
    "The dataset set we will use here is the Github-issues dataset from Kaggle. It contains over 5 million issue titles and descriptions from the year 2017. However, we can unfortunately not use all of them, due to limited resources. \n",
    "Our aim is, as before, to create a summary from a given input. \n",
    "\n",
    "https://www.kaggle.com/davidshinn/github-issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and exploring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5332153, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load csv file using pandas.\n",
    "file_path = './github_issues.csv'\n",
    "data = pd.read_csv(file_path, encoding='utf-8')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>issue_url</th>\n",
       "      <th>issue_title</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"https://github.com/zhangyuanwei/node-images/i...</td>\n",
       "      <td>can't load the addon. issue to: https://github...</td>\n",
       "      <td>can't load the addon. issue to: https://github...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"https://github.com/Microsoft/pxt/issues/2543\"</td>\n",
       "      <td>hcl accessibility a11yblocking a11ymas mas4.2....</td>\n",
       "      <td>user experience: user who depends on screen re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"https://github.com/MatisiekPL/Czekolada/issue...</td>\n",
       "      <td>issue 1265: issue 1264: issue 1261: issue 1260...</td>\n",
       "      <td>┆attachments: &lt;a href= https:&amp; x2f;&amp; x2f;githu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"https://github.com/MatisiekPL/Czekolada/issue...</td>\n",
       "      <td>issue 1266: issue 1263: issue 1262: issue 1259...</td>\n",
       "      <td>gitlo = github x trello\\n---\\nthis board is no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"https://github.com/MatisiekPL/Czekolada/issue...</td>\n",
       "      <td>issue 1288: issue 1285: issue 1284: issue 1281...</td>\n",
       "      <td>┆attachments: &lt;a href= https:&amp; x2f;&amp; x2f;githu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           issue_url  \\\n",
       "0  \"https://github.com/zhangyuanwei/node-images/i...   \n",
       "1     \"https://github.com/Microsoft/pxt/issues/2543\"   \n",
       "2  \"https://github.com/MatisiekPL/Czekolada/issue...   \n",
       "3  \"https://github.com/MatisiekPL/Czekolada/issue...   \n",
       "4  \"https://github.com/MatisiekPL/Czekolada/issue...   \n",
       "\n",
       "                                         issue_title  \\\n",
       "0  can't load the addon. issue to: https://github...   \n",
       "1  hcl accessibility a11yblocking a11ymas mas4.2....   \n",
       "2  issue 1265: issue 1264: issue 1261: issue 1260...   \n",
       "3  issue 1266: issue 1263: issue 1262: issue 1259...   \n",
       "4  issue 1288: issue 1285: issue 1284: issue 1281...   \n",
       "\n",
       "                                                body  \n",
       "0  can't load the addon. issue to: https://github...  \n",
       "1  user experience: user who depends on screen re...  \n",
       "2  ┆attachments: <a href= https:& x2f;& x2f;githu...  \n",
       "3  gitlo = github x trello\\n---\\nthis board is no...  \n",
       "4  ┆attachments: <a href= https:& x2f;& x2f;githu...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "issue_url      0\n",
       "issue_title    0\n",
       "body           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for missing values.\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make the transition from the amazon review example to this one as comfortable as possbile,\n",
    "# we rename the columns. \n",
    "data.rename(index = str, columns = {'issue_title':'Summary', 'body':'Text'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see how long the texts and summaries are. \n",
    "len_summaries = [len(summary) for i, summary in enumerate(data.Summary)]\n",
    "len_texts = [len(text) for text in data.Text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(len_summaries).most_common(), Counter(len_texts).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as I said before we can not use all of the training examples. \n",
    "# to make training easier we will only use shorter texts (and summaries) of similar length.\n",
    "indices = [ind for ind, text in enumerate(data.Text) if 100 < len(text) < 108]\n",
    "raw_summaries = data.Summary[indices]\n",
    "raw_texts = data.Text[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78668, 78668, 78668)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(indices), len(raw_texts), len(raw_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# unfortunately the issues are in different languages. \n",
    "# to make learning easier to the model we will only use english ones. \n",
    "# for that we use langdetect.\n",
    "# with sufficient resources that might not be a problem, \n",
    "# but for our purposes it will be better. \n",
    "en_raw_summaries=[]\n",
    "en_raw_texts=[]\n",
    "\n",
    "for (s,t) in zip(raw_summaries, raw_texts):\n",
    "    try:\n",
    "        lang = detect(t)\n",
    "        if lang == 'en':\n",
    "            en_raw_summaries.append(s)\n",
    "            en_raw_texts.append(t)\n",
    "    except:\n",
    "        continue\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71508, 71508)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_raw_summaries), len(en_raw_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:\n",
      " the project seems interesting you should add a demo , cuz i almost lost interest when i didnt find one\n",
      "Summary:\n",
      " add a demo \n",
      "\n",
      "\n",
      "Text:\n",
      " this site should have a logo. probably nothing to special, but something small and neat would be cool.\n",
      "Summary:\n",
      " add a logo \n",
      "\n",
      "\n",
      "Text:\n",
      " page 8, i would add a ; at the end of the functions to avoid the copy/past error. very cosmetic, i know ;-\n",
      "Summary:\n",
      " adding a ; \n",
      "\n",
      "\n",
      "Text:\n",
      " description update element bulk docs to reflect all known limitations, expectations, and best practices\n",
      "Summary:\n",
      " box - bulk \n",
      "\n",
      "\n",
      "Text:\n",
      " 1.lock scan button after handling the plist. 2.alternative naming 3.version control for plist 4.ordering\n",
      "Summary:\n",
      " to do list \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t, s in zip(en_raw_texts[:5], en_raw_summaries[:5]):\n",
    "    print('Text:\\n', t,)\n",
    "    print('Summary:\\n', s, '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and prepare the data\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Time:  7.919019937515259\n"
     ]
    }
   ],
   "source": [
    "# preprocess the texts and summaries.\n",
    "# we have the option to keep_most or not. in this case we do not want 'to keep most', i.e. we will only keep\n",
    "# letters and numbers. \n",
    "# (to improve the model, this preprocessing step should be refined)\n",
    "processed_texts, processed_summaries, words_counted = summarizer_data_utils.preprocess_texts_and_summaries(\n",
    "    en_raw_texts[:20000],\n",
    "    en_raw_summaries[:20000],\n",
    "    keep_most=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['the',\n",
       "  'project',\n",
       "  'seems',\n",
       "  'interesting',\n",
       "  'you',\n",
       "  'should',\n",
       "  'add',\n",
       "  'a',\n",
       "  'demo',\n",
       "  'cuz',\n",
       "  'i',\n",
       "  'almost',\n",
       "  'lost',\n",
       "  'interest',\n",
       "  'when',\n",
       "  'i',\n",
       "  'didnt',\n",
       "  'find',\n",
       "  'one'],\n",
       " ['add', 'a', 'demo'],\n",
       " [('the', 18464),\n",
       "  ('to', 16602),\n",
       "  ('a', 8785),\n",
       "  ('in', 7344),\n",
       "  ('for', 6732),\n",
       "  ('is', 6575),\n",
       "  ('and', 6516),\n",
       "  ('it', 5234),\n",
       "  ('of', 5185),\n",
       "  ('i', 4813)])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a short look at what a processed text, summary etc look like. \n",
    "processed_texts[0], processed_summaries[0], words_counted[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create lookup dicts\n",
    "\n",
    "We cannot feed our network actual words, but numbers. So we first have to create our lookup dicts, where each words gets and int value (high or low, depending on its frequency in our corpus). Those help us to later convert the texts into numbers.\n",
    "\n",
    "We also add special tokens. EndOfSentence and StartOfSentence are crucial for the Seq2Seq model we later use.\n",
    "Pad token, because all summaries and texts in a batch need to have the same length, pad token helps us do that.\n",
    "\n",
    "So we need 2 lookup dicts:\n",
    " - From word to index \n",
    " - from index to word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15159 15159 15434\n"
     ]
    }
   ],
   "source": [
    "# create lookup dicts.\n",
    "# most oft the words only appear only once. \n",
    "# min_occureces set to 2 reduces our vocabulary by more than half.\n",
    "specials = [\"<EOS>\", \"<SOS>\",\"<PAD>\",\"<UNK>\"]\n",
    "word2ind, ind2word,  missing_words = summarizer_data_utils.create_word_inds_dicts(words_counted,\n",
    "                                                                                  specials = specials,\n",
    "                                                                                  min_occurences=2)\n",
    "print(len(word2ind), len(ind2word), len(missing_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained embeddings\n",
    "\n",
    "Optionally we can use pretrained word embeddings. Those have proved to increase training speed and accuracy.\n",
    "Here I used two different options. Either we use glove embeddings or embeddings from tf_hub.\n",
    "The ones from tf_hub worked better, so we use those. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove_embeddings_path = './glove.6B.300d.txt'\n",
    "# embedding_matrix_save_path = './embeddings/my_embedding_github.npy'\n",
    "# emb = summarizer_data_utils.create_and_save_embedding_matrix(word2ind,\n",
    "#                                                              glove_embeddings_path,\n",
    "#                                                              embedding_matrix_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the embeddings from tf.hub. \n",
    "embed = hub.Module(\"https://tfhub.dev/google/Wiki-words-250/1\")\n",
    "emb = embed([key for key in word2ind.keys()])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    embedding = sess.run(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./embeddings/my_embedding_github.npy', embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert text and summaries\n",
    "As I said before we cannot feed the words directly to our network, we have to convert them to numbers first of all. This is what we do here. And we also append the SOS and EOS tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts words in texts and summaries to indices\n",
    "converted_texts, unknown_words_in_texts = summarizer_data_utils.convert_to_inds(processed_texts,\n",
    "                                                                                word2ind,\n",
    "                                                                                eos = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_summaries, unknown_words_in_summaries = summarizer_data_utils.convert_to_inds(processed_summaries,\n",
    "                                                                                        word2ind,\n",
    "                                                                                        eos = True,\n",
    "                                                                                        sos = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4,\n",
       " 84,\n",
       " 230,\n",
       " 2241,\n",
       " 29,\n",
       " 21,\n",
       " 18,\n",
       " 6,\n",
       " 529,\n",
       " 13653,\n",
       " 13,\n",
       " 2306,\n",
       " 1828,\n",
       " 2994,\n",
       " 22,\n",
       " 13,\n",
       " 5531,\n",
       " 147,\n",
       " 92]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converted_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seems to have worked well. \n",
    "for t, s in zip(converted_texts[:5], converted_summaries[:5]):\n",
    "    print(summarizer_data_utils.convert_inds_to_text(t, ind2word),\n",
    "          summarizer_data_utils.convert_inds_to_text(s, ind2word))\n",
    "    print('\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model\n",
    "\n",
    "Now we can build and train our model. First we define the hyperparameters we want to use. Then we create our Summarizer and call the function .build_graph(), which as the name suggests, builds the computation graph. \n",
    "Then we can train the model using .train()\n",
    "\n",
    "After training we can try our model using .infer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "Unfortunately I do not have the resources to find the perfect (or right) hyperparameters, but these do pretty well. \n",
    "\n",
    "I trained the model for about 40 epochs. the training loss, as well as the validation loss were both still declining.\n",
    "I chose to use 90% of the data as trainign set and 10% as validation set. We could have also used sklearn's train_test_split here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model hyperparameters\n",
    "num_layers_encoder = 2\n",
    "num_layers_decoder = 2\n",
    "rnn_size_encoder = 500\n",
    "rnn_size_decoder = 500\n",
    "\n",
    "batch_size = 256\n",
    "epochs = 200\n",
    "clip = 5\n",
    "keep_probability = 0.8\n",
    "learning_rate = 0.0005\n",
    "max_lr=0.005\n",
    "learning_rate_decay_steps = 500\n",
    "learning_rate_decay = 0.90\n",
    "\n",
    "\n",
    "pretrained_embeddings_path = './embeddings/my_embedding_github.npy'\n",
    "summary_dir = os.path.join('./tensorboard/github_issues')\n",
    "\n",
    "\n",
    "use_cyclic_lr = True\n",
    "inference_targets=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(converted_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18000"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(20000*0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# build graph and train the model \n",
    "summarizer_model_utils.reset_graph()\n",
    "summarizer = Summarizer.Summarizer(word2ind,\n",
    "                                   ind2word,\n",
    "                                   save_path='./models/github_issues/my_model',\n",
    "                                   mode='TRAIN',\n",
    "                                   num_layers_encoder = num_layers_encoder,\n",
    "                                   num_layers_decoder = num_layers_decoder,\n",
    "                                   rnn_size_encoder = rnn_size_encoder,\n",
    "                                   rnn_size_decoder = rnn_size_decoder,\n",
    "                                   batch_size = batch_size,\n",
    "                                   clip = clip,\n",
    "                                   keep_probability = keep_probability,\n",
    "                                   learning_rate = learning_rate,\n",
    "                                   max_lr=max_lr,\n",
    "                                   learning_rate_decay_steps = learning_rate_decay_steps,\n",
    "                                   learning_rate_decay = learning_rate_decay,\n",
    "                                   epochs = epochs,\n",
    "                                   pretrained_embeddings_path = pretrained_embeddings_path,\n",
    "                                   use_cyclic_lr = use_cyclic_lr,)\n",
    "#                                    summary_dir = summary_dir)           \n",
    "\n",
    "summarizer.build_graph()\n",
    "summarizer.train(converted_texts[:18000], \n",
    "                 converted_summaries[:18000],\n",
    "                 validation_inputs=converted_texts[18000:],\n",
    "                 validation_targets=converted_summaries[18000:])\n",
    "\n",
    "# hidden training output.\n",
    "# both train and validation loss decrease nicely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "Now we can use our trained model to create summaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained embeddings.\n",
      "Graph built.\n",
      "INFO:tensorflow:Restoring parameters from ./models/github_issues/my_model\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "summarizer_model_utils.reset_graph()\n",
    "summarizer = Summarizer.Summarizer(word2ind,\n",
    "                                   ind2word,\n",
    "                                   './models/github_issues/my_model',\n",
    "                                   'INFER',\n",
    "                                   num_layers_encoder = num_layers_encoder,\n",
    "                                   num_layers_decoder = num_layers_decoder,\n",
    "                                   batch_size = len(converted_texts[:50]),\n",
    "                                   clip = clip,\n",
    "                                   keep_probability = 1.0,\n",
    "                                   learning_rate = 0.0,\n",
    "                                   beam_width = 5,\n",
    "                                   rnn_size_encoder = rnn_size_encoder,\n",
    "                                   rnn_size_decoder = rnn_size_decoder,\n",
    "                                   inference_targets = True,\n",
    "                                   pretrained_embeddings_path = pretrained_embeddings_path)\n",
    "\n",
    "summarizer.build_graph()\n",
    "preds = summarizer.infer(converted_texts[:50],\n",
    "                         restore_path =  './models/github_issues/my_model',\n",
    "                         targets = converted_summaries[:50])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "the project seems interesting you should add a demo cuz i almost lost interest when i didnt find one\n",
      "\n",
      "Actual Summary:\n",
      "add a demo\n",
      "\n",
      "Created Summary:\n",
      "add a demo demo\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "this site should have a logo probably nothing to special but something small and neat would be cool\n",
      "\n",
      "Actual Summary:\n",
      "add a logo\n",
      "\n",
      "Created Summary:\n",
      "add a logo logo\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "page 8 i would add a at the end of the functions to avoid the copy past error very cosmetic i know\n",
      "\n",
      "Actual Summary:\n",
      "adding a\n",
      "\n",
      "Created Summary:\n",
      "add more more\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "description update element bulk docs to reflect all known limitations expectations and best practices\n",
      "\n",
      "Actual Summary:\n",
      "box bulk\n",
      "\n",
      "Created Summary:\n",
      "salesforce bulk bulk bulk bulk\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "1 lock scan button after handling the plist 2 alternative naming 3 version control for plist 4 ordering\n",
      "\n",
      "Actual Summary:\n",
      "to do list\n",
      "\n",
      "Created Summary:\n",
      "do do to\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "in my opinion you should write hey https github com <UNK> test blob master second file py l1\n",
      "\n",
      "Actual Summary:\n",
      "typo hello\n",
      "\n",
      "Created Summary:\n",
      "some some issues issues\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "thank for your work i have a remote database that must be loaded with ssl how can i declare it <UNK>\n",
      "\n",
      "Actual Summary:\n",
      "use of ssl\n",
      "\n",
      "Created Summary:\n",
      "how to use heroku\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "last tag 0 1 0 is outdated current master version seems to be pretty stable what about adding new tag\n",
      "\n",
      "Actual Summary:\n",
      "add new tag\n",
      "\n",
      "Created Summary:\n",
      "add new new version version\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "hi not really an issue but how does this repo relate to https github com cornell lic spf thanks\n",
      "\n",
      "Actual Summary:\n",
      "amr v s spf\n",
      "\n",
      "Created Summary:\n",
      "amr <UNK> <UNK> spf\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "is there any way to get game id while is running or <UNK> start i need it for show match stats on my website\n",
      "\n",
      "Actual Summary:\n",
      "get game id\n",
      "\n",
      "Created Summary:\n",
      "error game game\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "should have a getting started page how to page that gives some basic instructions about what is expected\n",
      "\n",
      "Actual Summary:\n",
      "how to page\n",
      "\n",
      "Created Summary:\n",
      "how about about\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "tried from reddit https www reddit com r arma comments <UNK> lua scripting in arma but how do i use\n",
      "\n",
      "Actual Summary:\n",
      "how to use\n",
      "\n",
      "Created Summary:\n",
      "how to use\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "play needs a readme at the very least it should outline how we ship it a roadmap would be nice too\n",
      "\n",
      "Actual Summary:\n",
      "play readme\n",
      "\n",
      "Created Summary:\n",
      "update readme readme\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "1 repo pick name reply with you have contributors and <UNK> <UNK> 2 repo pick name search term 45\n",
      "\n",
      "Actual Summary:\n",
      "repo pick 3\n",
      "\n",
      "Created Summary:\n",
      "repo repo repo repo\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "in addition to the coordinates should we send back an sf object i e a tibble with a geometry column\n",
      "\n",
      "Actual Summary:\n",
      "sf objects\n",
      "\n",
      "Created Summary:\n",
      "sf chart chart\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "in model py what is this line doing exactly iu np diag hist hist sum 1 hist sum 0 np diag hist\n",
      "\n",
      "Actual Summary:\n",
      "what is iu\n",
      "\n",
      "Created Summary:\n",
      "what is iu\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "this is the body and here there are 3 subtasks 1 the first tasks 2 the second tasks 3 the third one\n",
      "\n",
      "Actual Summary:\n",
      "a nice title\n",
      "\n",
      "Created Summary:\n",
      "a nice title\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "dos plot is the next plan here are two concerns 1 total dos 2 <UNK> dos 3 atomic dos 4 spin up down\n",
      "\n",
      "Actual Summary:\n",
      "add dos plot\n",
      "\n",
      "Created Summary:\n",
      "add dos plot plot\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "bootstrap navigation bar w logo input field clear message board button checkboxes for page options\n",
      "\n",
      "Actual Summary:\n",
      "add to html\n",
      "\n",
      "Created Summary:\n",
      "add header header\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "just a reminder after completion of debugging back out https github com <UNK> labs <UNK> pull 944\n",
      "\n",
      "Actual Summary:\n",
      "back out 944\n",
      "\n",
      "Created Summary:\n",
      "back back 944\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "the code may be slightly messy make sure variable names are descriptive methods aren t left unused etc\n",
      "\n",
      "Actual Summary:\n",
      "cleanup code\n",
      "\n",
      "Created Summary:\n",
      "cleanup code code\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "expected behavior actual behavior information about the issue steps to reproduce the behavior 1 2\n",
      "\n",
      "Actual Summary:\n",
      "don t get it\n",
      "\n",
      "Created Summary:\n",
      "<UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "iteration partials where possible try to make this a ruby project instead of html might not be possible\n",
      "\n",
      "Actual Summary:\n",
      "dry up views\n",
      "\n",
      "Created Summary:\n",
      "dry up views views\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "error <UNK> too many symbolic links encountered open blur monitor src assets img app apps <UNK> svg\n",
      "\n",
      "Actual Summary:\n",
      "error on mac\n",
      "\n",
      "Created Summary:\n",
      "error on on on\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "lstm is applying the output transform after gating but it should be gating after the output transform\n",
      "\n",
      "Actual Summary:\n",
      "fix lstm bug\n",
      "\n",
      "Created Summary:\n",
      "fix lstm in in\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "i noticed the original pool this was built for is down will this work with other pools such as <UNK>\n",
      "\n",
      "Actual Summary:\n",
      "make it work\n",
      "\n",
      "Created Summary:\n",
      "is is the the\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "we must get rid of mark forever we need to stop him <UNK> help me mark help me we need to stop him\n",
      "\n",
      "Actual Summary:\n",
      "mark is dumb\n",
      "\n",
      "Created Summary:\n",
      "mark not dumb\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "data analysis statistics machine learning they are such huge concepts so do not jump in <UNK>\n",
      "\n",
      "Actual Summary:\n",
      "todo backend\n",
      "\n",
      "Created Summary:\n",
      "<UNK> <UNK> <UNK>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "hi <UNK> <UNK> <UNK> how can simulate a 3d noc with noxim is noxim support tsv links thanks a lot\n",
      "\n",
      "Actual Summary:\n",
      "3d noc suport\n",
      "\n",
      "Created Summary:\n",
      "3d noc suport\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "could a license be added to this repository the licensing is unclear given that there is no license file\n",
      "\n",
      "Actual Summary:\n",
      "add a license\n",
      "\n",
      "Created Summary:\n",
      "add license license\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "hello wave please add me as a collaborator to this repo please add me new to github <UNK> heart\n",
      "\n",
      "Actual Summary:\n",
      "add me please\n",
      "\n",
      "Created Summary:\n",
      "please add me lagos nigeria\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "currently only javascript is linted but css less needs to be too add stylelint package configure it\n",
      "\n",
      "Actual Summary:\n",
      "add stylelint\n",
      "\n",
      "Created Summary:\n",
      "add stylelint stylelint\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "is it possible to specify input and output process streams so as to use axel over an ssh server e g <UNK>\n",
      "\n",
      "Actual Summary:\n",
      "axel over ssh\n",
      "\n",
      "Created Summary:\n",
      "axel via via\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "ra currently has logs for cash shop i think we should be able to view it on flux just a suggestion\n",
      "\n",
      "Actual Summary:\n",
      "cash shop log\n",
      "\n",
      "Created Summary:\n",
      "cash shop shop\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "http <UNK> <UNK> <UNK> cz arch <UNK> man mc 1 500 internal server error the web application crashed\n",
      "\n",
      "Actual Summary:\n",
      "crash on mc 1\n",
      "\n",
      "Created Summary:\n",
      "crash on mc mc\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "https github com <UNK> algo blob master src routing route java <UNK> same x coordinates lead to div by 0\n",
      "\n",
      "Actual Summary:\n",
      "division by 0\n",
      "\n",
      "Created Summary:\n",
      "division <UNK> <UNK>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "getting an error on load of <UNK> please include forge 13 19 0 <UNK> or above i am using 13 20 0 <UNK>\n",
      "\n",
      "Actual Summary:\n",
      "error on load\n",
      "\n",
      "Created Summary:\n",
      "error error error\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "fix issues from travis build 28 https travis ci org hfagerlund hfagerlund github io jobs <UNK>\n",
      "\n",
      "Actual Summary:\n",
      "fix the build\n",
      "\n",
      "Created Summary:\n",
      "ci ci failed failed\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "write a program using <UNK> to display the following output hello world <UNK> software engineering\n",
      "\n",
      "Actual Summary:\n",
      "hello world 2\n",
      "\n",
      "Created Summary:\n",
      "write on on\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "the contents of the home view should viewable in list or map views currently we only have list view\n",
      "\n",
      "Actual Summary:\n",
      "home map view\n",
      "\n",
      "Created Summary:\n",
      "view view view\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "not sure if issue sorry how do i make an ultra split so when i split i merge before 0 what is merge time\n",
      "\n",
      "Actual Summary:\n",
      "how do i make\n",
      "\n",
      "Created Summary:\n",
      "how do i make make\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "i cant even install it because of the file it is in do you think you could make it a zip file plz\n",
      "\n",
      "Actual Summary:\n",
      "i cant run it\n",
      "\n",
      "Created Summary:\n",
      "is is a a\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "when trying to kick the puck it doesn t move this is what shows on the console http prntscr com <UNK>\n",
      "\n",
      "Actual Summary:\n",
      "ice <UNK> bug\n",
      "\n",
      "Created Summary:\n",
      "ice <UNK> <UNK> <UNK>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "write the definition of a value returning function that inputs data into a variable of type tourtype\n",
      "\n",
      "Actual Summary:\n",
      "lab 2 task 4\n",
      "\n",
      "Created Summary:\n",
      "lab lab d d\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "recently i faced a need to implement login as user function any ideas how to do this on top of php auth\n",
      "\n",
      "Actual Summary:\n",
      "login as user\n",
      "\n",
      "Created Summary:\n",
      "login login login\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "this metric needs to be updated for all years but there are likely two methods 2014 2016 and 2011 2013\n",
      "\n",
      "Actual Summary:\n",
      "<UNK> bf volume\n",
      "\n",
      "Created Summary:\n",
      "<UNK> bf bf volume volume\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "hi as per php 7 requirements short tags is removed ensure this package is compatible with php 7\n",
      "\n",
      "Actual Summary:\n",
      "php 7 support\n",
      "\n",
      "Created Summary:\n",
      "php php 7 7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "todo push our api docs somewhere public on cd see readme for the commands to create the documentation\n",
      "\n",
      "Actual Summary:\n",
      "push api docs\n",
      "\n",
      "Created Summary:\n",
      "push push api api\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "is there a way to save the image to file i need the file location and filename any idea how to do that\n",
      "\n",
      "Actual Summary:\n",
      "save to file\n",
      "\n",
      "Created Summary:\n",
      "save for for file file\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "write your readme comments here https hackmd io <UNK>\n",
      "\n",
      "Actual Summary:\n",
      "set up readme\n",
      "\n",
      "Created Summary:\n",
      "<UNK> <UNK> <UNK>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show results. \n",
    "summarizer_model_utils.sample_results(preds,\n",
    "                                      ind2word,\n",
    "                                      word2ind,\n",
    "                                      converted_summaries[:50],\n",
    "                                      converted_texts[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Generally I am really impressed by how well the model works. \n",
    "We only used a limited amount of data, trained it for a limited amount of time and used nearly random hyperparameters and it still delivers good results. \n",
    "\n",
    "However, we are clearly overfitting the training data and the model does not perfectly generalize.\n",
    "Sometimes the summaries the model creates are good, sometimes bad, sometimes they are better than the original ones and sometimes they are just really funny.\n",
    "\n",
    "\n",
    "Therefore it would be really interesting to scale it up and see how it performs. \n",
    "\n",
    "To sum up, I am impressed by seq2seq models, they perform great on many different tasks and I look foward to exploring more possible applications. \n",
    "(speech recognition...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
